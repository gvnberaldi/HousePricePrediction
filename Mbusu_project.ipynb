{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9PcpQXLT9D0s"
   },
   "source": [
    "# Machine Learning Project \n",
    "### Mbusu-Team \n",
    "-  **House Prices Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XHVXQCXf9D07"
   },
   "source": [
    "### **Data Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "import sys\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from itertools import groupby\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from pandas.api.types import is_integer_dtype\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statistics import mean\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "hue_order = [\"HIGH\",\"MEDIUM\",\"LOW\"]\n",
    "color = [\"#BF4E30\",\"#2E86AB\",\"#E7A012\"]\n",
    "palette = {\n",
    "    'LOW': color[0],\n",
    "    'MEDIUM': color[1],\n",
    "    'HIGH': color[2],\n",
    "}\n",
    "TARGET_FEATURE = 'SalePrice'\n",
    "TARGET_FEATURE_CONTI = 'salePriceNum'\n",
    "TARGET_FEATURE_VALUES = ['LOW', 'MEDIUM', 'HIGH']\n",
    "SEED = 6536"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_category(df: pd.DataFrame, category: str,  attribute: str, exclude:list, target_feature = TARGET_FEATURE):\n",
    "    df_filtered = df[df[target_feature] == category]\n",
    "    df_filtered = df_filtered[~df_filtered[attribute].isin(exclude)]\n",
    "    return df_filtered[attribute].mode().iloc[0]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information Gain function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IG(df, target, attribute):\n",
    "\n",
    "    df_sub = []\n",
    "\n",
    "    for value in df[attribute].unique():\n",
    "        df_sub.append( df[df[attribute] == value] )\n",
    "        \n",
    "    counts = df[target].value_counts()\n",
    "    probs = counts / len(df)\n",
    "    e = -np.sum(probs * np.log2(probs))\n",
    "\n",
    "    information_gain = e \n",
    "    for df_s in df_sub:\n",
    "        counts = df_s[target].value_counts()\n",
    "        probs = counts / len(df_s)\n",
    "        e_s = -np.sum(probs * np.log2(probs))\n",
    "        information_gain -= len(df_s) / len(df) * e_s\n",
    "\n",
    "    return information_gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_attribute(df, attribute):\n",
    "    mean_low = df[ ( df[\"SalePrice\"] == 'LOW' ) & ( df[attribute] != 'NA' ) ][attribute].astype(\"int\").mean()\n",
    "    mean_medium = df[ ( df[\"SalePrice\"] == 'MEDIUM' ) & ( df[attribute] != 'NA' ) ][attribute].astype(\"int\").mean()\n",
    "    mean_high = df[ ( df[\"SalePrice\"] == 'HIGH' ) & ( df[attribute] != 'NA' ) ][attribute].astype(\"int\").mean()\n",
    "    return (mean_low, mean_medium, mean_high)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_means(df,attribute):\n",
    "    mean_values = mean_attribute(df,attribute)\n",
    "\n",
    "    for ind , e  in df.iterrows():\n",
    "        if e[attribute] == \"NA\":\n",
    "            if(e[\"SalePrice\"] == \"LOW\"):\n",
    "                df.at[ind, attribute] = mean_values[0]\n",
    "            elif e[\"SalePrice\"] == \"MEDIUM\":\n",
    "                df.at[ind, attribute] = mean_values[1]\n",
    "            else:\n",
    "                df.at[ind, attribute] = mean_values[2]\n",
    "\n",
    "\n",
    "    df[attribute] = df[attribute].astype(\"int\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate categories if below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, new_name, threshold, attribute):\n",
    "    values_count= df[attribute].value_counts()\n",
    "    for i in values_count.keys():\n",
    "        if values_count[i] <= threshold:\n",
    "            df[attribute] = df[attribute].replace([i], new_name)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot frequencies and percentages for numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencies_and_percentages_numeric(df, target, attribute, bin, rotation=0, figsize=(15,5),display=False):\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize)\n",
    "    df1 = df.copy()\n",
    "    df1[attribute] = pd.qcut(df1[attribute], q=bin)\n",
    "    regexp_1 = re.compile(r\".(\\-?\\d+)\\.?\\d*, ?(\\-?\\d+)\\.?\\d*.\")\n",
    "    for i in df1[attribute].unique():\n",
    "        re_match = regexp_1.match(str(i))\n",
    "        if re_match is not None:\n",
    "            val = [re_match[1], re_match[2]]\n",
    "            for j in range(0, len(val)):\n",
    "                    if val[j] == \"-0\":\n",
    "                         val[j] = \"0\"\n",
    "            df1[attribute] = df1[attribute].replace(i, str(val[0])+\" - \"+str(val[1]))\n",
    "    dfp = df1[[attribute,target]].pivot_table(index = attribute, columns=[target],  aggfunc=len)\n",
    "    dfp = dfp.fillna(0)\n",
    "    dfpp = dfp.pipe(lambda d : 100*d.div(sum(d[v] for v in df1[target].unique()), axis='index'))\n",
    "    if display:\n",
    "        display(dfpp)\n",
    "    dfp.plot(kind='bar', ax=ax[0]  , title = f\"Count by {attribute}\" ,  color = color)\n",
    "    dfpp.plot(kind='bar',ax=ax[1] , stacked=True , title  = f\"Percentage by {attribute}\", legend = False , color = color )\n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=rotation, horizontalalignment='right')\n",
    "    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=rotation, horizontalalignment='right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot frequencies and percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencies_and_percentages(df, target, attribute, figsize=(15,5), rotation=0, display=False):\n",
    "    fig, ax = plt.subplots(1,2, figsize=figsize)\n",
    "\n",
    "    dfp = df[[attribute,target]].pivot_table(index = attribute, columns=[target],  aggfunc=len)\n",
    "    dfp = dfp.fillna(0)\n",
    "    dfpp = dfp.pipe(lambda d : 100*d.div(sum(d[v] for v in df[target].unique()), axis='index'))\n",
    "    if display:\n",
    "        display(dfpp)\n",
    "    dfp.plot(kind='bar', ax=ax[0]  , title = f\"Count by {attribute}\" ,  color = color)\n",
    "    dfpp.plot(kind='bar',ax=ax[1] , stacked=True , title  = f\"Percentage by {attribute}\", legend = False , color = color )\n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=rotation, horizontalalignment='right')\n",
    "    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=rotation, horizontalalignment='right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_common_analysis(data, x, target ,palette, histplot = True, percentages=True, count = None, display=True, figsize=(15,5), rotation = 0):\n",
    "    if histplot:\n",
    "        plt.figure(figsize = figsize)\n",
    "        sb.histplot(x = data[x], multiple='stack', hue = target,  data = data, hue_order = hue_order, palette = palette, alpha=1)\n",
    "    if percentages:\n",
    "        frequencies_and_percentages(data, target, x,figsize=figsize,rotation=rotation)\n",
    "    if display:\n",
    "        if count is None:\n",
    "            print(data[x].value_counts())\n",
    "        else :\n",
    "            print(data[x].value_counts()[count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def histplot(df,target, attribute, bins = 200 , figsize=(15,5), xticks_rot=0, xlim = None, multiple = 'layer', alpha = 0.5):\n",
    "    plt.figure(figsize = figsize)\n",
    "    sb.histplot(x = df[attribute], hue = target, data = df, kde=True , hue_order = hue_order, palette = palette, multiple= multiple , \n",
    "                bins = bins ,alpha = alpha , common_bins= True)\n",
    "    plt.xticks(rotation=xticks_rot)\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Indepence_test_categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line(ax, xpos, ypos):\n",
    "    line = plt.Line2D([xpos, xpos], [ypos + .1, ypos],\n",
    "                      transform=ax.transAxes, color='gray')\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "\n",
    "def label_len(my_index,level):\n",
    "    labels = my_index.get_level_values(level)\n",
    "    return [(k, sum(1 for i in g)) for k,g in groupby(labels)]\n",
    "    \n",
    "def label_group_bar_table(ax, df):\n",
    "    ypos = -.05\n",
    "    scale = 1./df.index.size\n",
    "    for level in range(df.index.nlevels)[::-1]:\n",
    "        pos = 0\n",
    "        for label, rpos in label_len(df.index,level):\n",
    "            lxpos = (pos + .5 * rpos)*scale\n",
    "            ax.text(lxpos, ypos, label, ha='center', transform=ax.transAxes)\n",
    "            if pos % 3 == 0: add_line(ax, pos*scale, ypos - .052)\n",
    "            pos += rpos\n",
    "        add_line(ax, pos*scale , ypos - .052)\n",
    "        ypos -= .05\n",
    "\n",
    "def barplot_diffvalues(dataset, feature_name):\n",
    "    df = dataset.reset_index().groupby([feature_name, TARGET_FEATURE]).sum()\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #print(dataset)\n",
    "    dataset.pivot_table(index = [feature_name, TARGET_FEATURE], columns=['__Type']).plot(kind='bar', ax=ax, width=.7)\n",
    "    \n",
    "    labels = ['' for item in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title('Expected/Observed Frequencies')\n",
    "    ax.set_xlabel(f'{feature_name}/{TARGET_FEATURE}', labelpad=40)\n",
    "    ax.set_ylabel(f'{TARGET_FEATURE}')\n",
    "    plt.legend(['Expected', 'Observed'])\n",
    "    label_group_bar_table(ax, df)\n",
    "\n",
    "    fig.subplots_adjust(bottom=.1*df.index.nlevels)\n",
    "    plt.show()\n",
    "\n",
    "def stattest_quali(df, feature_name: str, target = TARGET_FEATURE, fig_width=15, fig_height=3, typeplot = 'heatmap', silent: bool=False, cramer: bool=False):\n",
    "    #\n",
    "    # chi-square test and expected frequencies matrix\n",
    "    #\n",
    "    crosstab = pd.crosstab(df[feature_name], df[target])\n",
    "    chi_square_args = crosstab.values\n",
    "    _, p_value, _, _ = stats.chi2_contingency(chi_square_args)\n",
    " \n",
    "    #\n",
    "    # Cramer's V\n",
    "    #\n",
    "    cramer_crosstab = crosstab.values\n",
    "    X2 = stats.chi2_contingency(cramer_crosstab, correction=False)[0]\n",
    "    N = np.sum(cramer_crosstab)\n",
    "    minimum_dimension = min(cramer_crosstab.shape)-1\n",
    "    corr_cramer = np.sqrt((X2/N) / minimum_dimension)\n",
    "\n",
    "    expf_crosstab = pd.DataFrame(crosstab)\n",
    "    for idx in expf_crosstab.index:\n",
    "        for col in expf_crosstab.columns:\n",
    "            expf_crosstab.at[idx, col] = crosstab.loc[idx].values.sum() * crosstab[col].values.sum() / crosstab.values.sum()\n",
    "\n",
    "    if not silent:\n",
    "        #\n",
    "        # observed/expected frequencies heatmaps\n",
    "        #\n",
    "        fig_width = min(fig_width, 1.5 * len(df[feature_name].unique()))\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(fig_width, fig_height * 2))\n",
    "        sb.heatmap(crosstab.T, ax=ax[0], annot=True, cmap='coolwarm', fmt='.2f')\n",
    "        sb.heatmap(expf_crosstab.T, ax=ax[1], annot=True, cmap='coolwarm', fmt='.2f')\n",
    "        ax[0].set_title('Observed Frequencies')    \n",
    "        ax[1].set_title('Expected Frequencies')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if(typeplot == 'heatmap'):\n",
    "            \n",
    "            #\n",
    "            # frequencies difference heatmap\n",
    "            #\n",
    "            obsexp_diff_df = crosstab - expf_crosstab\n",
    "            fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "            plt.title(\"Frequency Differences (Observed - Expected)\")\n",
    "            sb.heatmap(obsexp_diff_df.T, annot=True, cmap='vlag', fmt='.2f')\n",
    "        \n",
    "        elif typeplot == 'barplot':\n",
    "            #\n",
    "            # observed/expected frequencies barplot\n",
    "            #\n",
    "            value_vars_x = [val + '_x' for val in df[target].unique().tolist()]\n",
    "            value_vars_y = [val + '_y' for val in df[target].unique().tolist()]\n",
    "            value_vars_xy = value_vars_x + value_vars_y\n",
    "            obs_exp_df = pd.merge(crosstab, expf_crosstab, on=[feature_name]).reset_index()\n",
    "            obs_exp_df = pd.melt(obs_exp_df, id_vars=[feature_name], value_vars=value_vars_xy)\n",
    "            obs_exp_df['__Type'] = obs_exp_df[target]\n",
    "\n",
    "            for val in value_vars_x: obs_exp_df.loc[obs_exp_df['__Type'] == val, '__Type'] = '__OBS'\n",
    "            for val in value_vars_y: obs_exp_df.loc[obs_exp_df['__Type'] == val, '__Type'] = '__EXP'\n",
    "\n",
    "            for price in df[target].unique().tolist():\n",
    "                for obs_exp in ['x', 'y']:\n",
    "                    obs_exp_df.loc[obs_exp_df[target] == price + '_' + obs_exp, target] = price[0]\n",
    "\n",
    "            barplot_diffvalues(obs_exp_df, feature_name)\n",
    "\n",
    "    #\n",
    "    # chi-square test outcome\n",
    "    #\n",
    "    p_value_info = '[<0.05]' if p_value < 0.05 else ''\n",
    "    if not silent:\n",
    "        print(\"Chi-square Test - Outcome\")\n",
    "        print(\"P-Value:\", p_value, p_value_info)\n",
    "        print(\"Cramer's V - Outcome\")\n",
    "        print(\"Coeff:\", corr_cramer)\n",
    "    if cramer:\n",
    "        return p_value, p_value_info, corr_cramer\n",
    "    return p_value, p_value_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Indepence_test_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stattest_quanti(df, feature_name: str, target = TARGET_FEATURE ,low_limit=0, upper_limit=100, silent = False, cramer: bool = False):\n",
    " \n",
    "    #\n",
    "    # Cramer's V\n",
    "    #\n",
    "    if cramer:\n",
    "        df_copy2 = df.copy(deep=True)\n",
    "        df_copy2[feature_name] = pd.qcut(df_copy2[feature_name], q=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], duplicates='drop', labels=False)\n",
    "        if len(df_copy2[feature_name].unique().tolist()) >= 2:\n",
    "            _, _, corr_cramer = stattest_quali(df_copy2, feature_name=feature_name, target=target, cramer=True, silent=True)\n",
    "        else:\n",
    "            corr_cramer = 0.0 # TODO: raise RuntimeWarning(\"Computing Cramer's V coefficient but %s admits just one categorical value.\" % feature_name)\n",
    "        \n",
    "    #\n",
    "    # plot means w.r.t. target categories\n",
    "    #\n",
    "    df_conti_origin = df[[feature_name, target, TARGET_FEATURE_CONTI]] if ( target == TARGET_FEATURE) else None\n",
    "    df_categ_origin = df[[feature_name, target]]\n",
    "    target_feature_values = df[target].unique().tolist()\n",
    "    df = df_categ_origin.groupby(target)\n",
    "    mean_df = df.mean().reset_index().rename(columns={feature_name: feature_name + ' (AVG)'})\n",
    "\n",
    "    if not silent: \n",
    "        num = 3\n",
    "        if df_conti_origin is None:\n",
    "            num = 2\n",
    "        _, ax = plt.subplots(1, num, figsize=(20, 5))\n",
    "\n",
    "        mean_df.plot(kind='bar', x=target, ax=ax[0])\n",
    "        for container in ax[0].containers:\n",
    "            ax[0].bar_label(container, fmt='%.1f')\n",
    "        ax[0].set_ylim(low_limit, upper_limit)\n",
    "        ax[0].tick_params(labelrotation=0)\n",
    "        sb.histplot(x=feature_name, hue=target, data=df_categ_origin, kde=True, ax=ax[1])\n",
    "        for tval in target_feature_values:\n",
    "            ax[1].axvline(x=df_categ_origin[df_categ_origin[target] == tval][feature_name].mean(), color='gray', ls='--', lw=1.5)\n",
    "        \n",
    "        if df_conti_origin is not None : sb.regplot(x=feature_name, y=TARGET_FEATURE_CONTI, data=df_conti_origin, line_kws={\"color\": \"darkred\"}, ax=ax[2])\n",
    "        \n",
    "    #\n",
    "    # perform ANOVA test\n",
    "    #\n",
    "    anova_args = tuple(df[feature_name].apply(list).reset_index()[feature_name])\n",
    "    f_statistic, p_value = stats.f_oneway(*anova_args)\n",
    "\n",
    "    p_value_info = '[<0.05]' if p_value < 0.05 else ''\n",
    "    if not silent:\n",
    "        print(\"ANOVA Test - Outcome\")\n",
    "        print(\"P-Value:\", p_value, p_value_info)\n",
    "        if cramer:\n",
    "            print(\"Cramer's V - Outcome\")\n",
    "            print(\"Coeff:\", corr_cramer)\n",
    "    if cramer:\n",
    "        return p_value, p_value_info, corr_cramer\n",
    "    return p_value, p_value_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Numeric-Numeric Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, n1, n2, silent = False):\n",
    "\n",
    "    corr, p_values = stats.pearsonr(df[n1], df[n2])\n",
    "    \n",
    "    if not silent:\n",
    "        sb.regplot(x=n1, y=n2, data=df, line_kws={\"color\": \"darkred\"})\n",
    "        print(corr, p_values)\n",
    "\n",
    "    return (corr, p_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Categoric Ordinal Plots & Stattest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_ordinal_plot(df, attr, test_typeplot='heatmap'):\n",
    "\n",
    "    attr_values = df[attr].unique().tolist()\n",
    "    df[attr] = df[attr].astype('object')\n",
    "    for val in attr_values: df.loc[df[attr] == val, attr] = attr + '_' + str(val)\n",
    "    df[attr] = df[attr].astype('category')\n",
    "\n",
    "    frequencies_and_percentages(df, TARGET_FEATURE, attr)\n",
    "    stattest_quali(df, attr, typeplot=test_typeplot)\n",
    "    print(IG(df, TARGET_FEATURE, attr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand(dataf: pd.DataFrame, feature_name: str, dtype: str='category', \\\n",
    "               predconf_max: float=1.0, predconf_min: float=0.33, showplots=True, na = 'NA', bins = 50, \\\n",
    "               low_limit = 0, upper_limit = 100 , xlim = None, rotation = 0):\n",
    "    if dtype == 'object':\n",
    "        dataf = dataf.copy(deep=True)\n",
    "    data_feature = dataf[feature_name]\n",
    "    data_target = dataf[TARGET_FEATURE]\n",
    "    report = ''\n",
    "    report += \"Understanding %s:\" % feature_name + \"\\n\"\n",
    "    \n",
    "    #\n",
    "    # count missing values\n",
    "    #\n",
    "    miss_values = data_feature.isna().sum()\n",
    "    miss_values += dataf[data_feature == na ].index.size\n",
    "    report += \"\\tMissing values: \" + str(miss_values) + \"\\n\"\n",
    "\n",
    "    data_feature = data_feature.astype(dtype)\n",
    "\n",
    "    if dtype == 'category' or dtype == 'object':\n",
    "\n",
    "        if dtype == 'object':\n",
    "            attr_values = data_feature.unique().tolist()\n",
    "            data_feature = data_feature.astype('object')\n",
    "            for ind in range(len(dataf)): data_feature[ind] = feature_name + '_' + str(data_feature[ind])\n",
    "            data_feature = data_feature.astype('category')\n",
    "            dataf[feature_name] = data_feature\n",
    "\n",
    "        if showplots: \n",
    "            frequencies_and_percentages(dataf, TARGET_FEATURE, feature_name, rotation= rotation)\n",
    "        #\n",
    "        # perform the independence test\n",
    "        #\n",
    "        p_value, p_value_info = stattest_quali(dataf, feature_name, typeplot='heatmap' if showplots else'none', silent=not showplots)\n",
    "        report += \"\\tIndependence Test: \" + '{:.2e}'.format(p_value) + p_value_info + \"\\n\"\n",
    "\n",
    "        #\n",
    "        # compute the IG\n",
    "        #\n",
    "        ig = IG(dataf, TARGET_FEATURE, feature_name)\n",
    "        report += \"\\tInformation Gain: \" + \"%.2f\" % ig + \"\\n\"\n",
    "\n",
    "        crosstab = pd.crosstab(data_feature, data_target)\n",
    "        to_predict_map = {}\n",
    "        no_predict_values = []\n",
    "        for cat_val in crosstab.T:\n",
    "            no_predict_values.append(cat_val)\n",
    "        predconf = predconf_max\n",
    "\n",
    "        while predconf > predconf_min and len(no_predict_values) > 0:\n",
    "            new_no_predict_values = []\n",
    "\n",
    "            for cat_val in no_predict_values:\n",
    "                to_predict = []\n",
    "                for target_val in TARGET_FEATURE_VALUES:\n",
    "                    if crosstab.at[cat_val, target_val] / crosstab.loc[cat_val].values.sum() >= predconf:\n",
    "                        to_predict.append(target_val)\n",
    "                        to_predict.append(predconf)\n",
    "                        break\n",
    "                \n",
    "                if len(to_predict) > 0:\n",
    "                    new_to_predict_map = to_predict_map.copy()\n",
    "                    added = False\n",
    "                    to_delete = []\n",
    "                    for k in to_predict_map.keys():\n",
    "                        if to_predict_map[k] == to_predict:\n",
    "                            new_to_predict_map[k + '+' + cat_val] = to_predict\n",
    "                            added = True\n",
    "                            to_delete.append(k)\n",
    "                    if not added:\n",
    "                        new_to_predict_map[cat_val] = to_predict\n",
    "                    for k in to_delete:\n",
    "                        del new_to_predict_map[k]\n",
    "                    to_predict_map = new_to_predict_map\n",
    "                else:\n",
    "                    new_no_predict_values.append(cat_val)\n",
    "            \n",
    "            no_predict_values = new_no_predict_values\n",
    "            predconf -= 0.05\n",
    "        \n",
    "        report += \"\\tWhen the %s is:\" % feature_name + \"\\n\"\n",
    "        for cat_vals in to_predict_map.keys():\n",
    "            if to_predict_map[cat_vals][1] == 1.0:\n",
    "                report += \"\\t\\t• %s, then the sale price is %s\" % (cat_vals, to_predict_map[cat_vals][0])\n",
    "            else:\n",
    "                report += \"\\t\\t• %s, then the sale price is %.0f%% likely to be %s\" % \\\n",
    "                    (cat_vals, to_predict_map[cat_vals][1]*100, to_predict_map[cat_vals][0])\n",
    "            \n",
    "            cat_vals_freq = 0.0\n",
    "            for cval in cat_vals.split('+'):\n",
    "                cat_vals_freq += len(dataf[data_feature == cval]) / len(data_feature)\n",
    "            report += \" (%.2f%% of the times)\" % (cat_vals_freq*100) + \"\\n\"\n",
    "\n",
    "        if len(no_predict_values) > 0:\n",
    "            report += \"\\t\\t• %s cannot predict the sale price\" % '+'.join(no_predict_values) + \"\\n\"\n",
    "        report += \"\\n\"\n",
    "\n",
    "    elif dtype == 'int':\n",
    "        if showplots: \n",
    "            histplot(df=dataf, target=TARGET_FEATURE, attribute=feature_name, bins= bins, xlim = xlim)\n",
    "        p_value, p_value_info = stattest_quanti(df=dataf, feature_name=feature_name, target=TARGET_FEATURE,low_limit=low_limit, upper_limit=upper_limit, silent=True)\n",
    "        report += \"\\tIndependence Test: \" + '{:.2e}'.format(p_value) + p_value_info + \"\\n\"\n",
    "        corr, p_value_corr = correlation(df=dataf, n1= feature_name, n2= TARGET_FEATURE_CONTI, silent=True)\n",
    "        report += \"\\tCorrelation Index: %.2f\" % corr + \"\\n\"\n",
    "        report += \"\\tCorrelation p_value: \" + '{:.2e}'.format(p_value_corr) + \"\\n\"\n",
    "\n",
    "    print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Undestanding all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_all(df):\n",
    "    features = df.columns\n",
    "    repos = []\n",
    "    for f in features:\n",
    "        if df[f].dtype == 'object':\n",
    "            repo, ig, p_value = understand(df, f, 'category')\n",
    "            repos.append((ig, p_value, repo))\n",
    "    repos.sort(reverse=True)\n",
    "    for r in repos:\n",
    "        print(r[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Total Square Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_square_feet(dataf: pd.DataFrame):\n",
    "    total_sf = []\n",
    "    for ind, e in dataf.iterrows():\n",
    "        totalsf = e['TotalBsmtSF']\n",
    "        if e[\"GarageType\"] != 'Basment': \n",
    "            totalsf += e['GarageArea']\n",
    "        # totalsf += e['1stFlrSF']\n",
    "        # totalsf += e['2ndFlrSF']\n",
    "        totalsf += e['GrLivArea']\n",
    "        total_sf.append(totalsf)\n",
    "    \n",
    "    return total_sf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df): \n",
    "        attributes = df.columns.tolist()\n",
    "        attributes.remove(\"SalePrice\")\n",
    "\n",
    "        cat = list(df.select_dtypes(include = ['category', 'object']).columns)\n",
    "        cat_index = []\n",
    "        for i in cat:\n",
    "                cat_index.append(df.columns.get_loc(i))\n",
    "        #print(\"Length of cat attr: \" + str(len(cat_index)))\n",
    "        X_resample, y_resampled = SMOTENC(categorical_features=cat_index, sampling_strategy=\"minority\", random_state=1020).fit_resample(df[attributes], df[TARGET_FEATURE])\n",
    "\n",
    "        df_resample = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "\n",
    "        df_resample[attributes] = X_resample\n",
    "        df_resample[TARGET_FEATURE] = y_resampled\n",
    "        return df_resample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house-prices-advanced-regression-techniques/train.csv', keep_default_na = False, low_memory= False )\n",
    "df[\"salePriceNum\"] = df.SalePrice.rename(\"salePriceNum\")\n",
    "df[\"SalePrice\"] = pd.cut(df[\"salePriceNum\"], bins = [0,150000,300000 - 1 ,int(sys.maxsize)], labels=[\"LOW\",\"MEDIUM\",\"HIGH\"])\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "# To remove\n",
    "df[\"TotalSF\"] = total_square_feet(df)\n",
    "\n",
    "df_resample = resample(df)\n",
    "\n",
    "df_resample =  df_resample.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SalePrice.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(x = 'SalePrice', multiple='stack', hue = \"SalePrice\",  data = df, hue_order = hue_order, palette = palette, alpha=1)\n",
    "df[\"SalePrice\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(x = 'SalePrice', multiple='stack', hue = \"SalePrice\",  data = df_resample, hue_order = hue_order, palette = palette, alpha=1)\n",
    "df_resample[\"SalePrice\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undestanding the effect of the attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"Id\"], bins= len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df,'SalePrice',\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Entropy ( Log n ) -> \" + str(math.log(len(df), 2)))\n",
    "\n",
    "counts = df['Id'].value_counts()\n",
    "probs = counts / len(df)\n",
    "e = -np.sum(probs * np.log2(probs))\n",
    "print(\"Entropy -> \"+ str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSSubClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[\"MSSubClass\"] = df_copy[\"MSSubClass\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_and_percentages(df_copy, 'SalePrice', 'MSSubClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG(df,\"SalePrice\",\"MSSubClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indepence test\n",
    "\n",
    "stattest_quali(df, 'MSSubClass')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSZoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name='MSZoning')   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_means(df_copy,'LotFrontage')\n",
    "histplot(df_copy,'SalePrice',\"LotFrontage\", multiple='layer', bins=50, alpha=0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LotArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the graphs, the l the lot area the higher the price of the house.\n",
    "# We think that it is useful to split the values of the attribute into bins.\n",
    "frequencies_and_percentages_numeric(df, 'SalePrice', 'LotArea', rotation=30, bin = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "understand(dataf=df_copy, feature_name='LotArea', dtype='int',showplots=False, upper_limit=20000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given that the training is strongly unbalanced\n",
    "df_copy['Street'] = df['Street'].astype('category')\n",
    "category_common_analysis(data=df_copy, x='Street',target='SalePrice',histplot=False, palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_common_analysis(data=df, x='Alley',target='SalePrice',percentages=False, palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LotShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to be verified with chi-squared test\n",
    "# IR2 and IR3 can be aggregated in a new category\n",
    "#we can see that in proportion R1 and Reg can be significantly different\n",
    "understand(dataf=df_copy,feature_name='LotShape')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LandContour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe not enough values for a good model, we do not know if they are significant\n",
    "category_common_analysis(data=df, x='LandContour',target='SalePrice', palette=palette, histplot=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the houses (except one) have all the utilies\n",
    "#Although it is considered to be a useful element, we cannot use this attribute.\n",
    "category_common_analysis(data=df, x='Utilities',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LotConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it can be seen that for some values there are not enough sample, however it seems that the other attributes have a similar distribution\n",
    "#The attribute can be removed since it would not be useful to the model\n",
    "\n",
    "category_common_analysis(data=df, x='LotConfig',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LandSlope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la pendenza del terreno non dovrebbe influenzare il livello della proprietà\n",
    "#Troppi pochi campioni\n",
    "category_common_analysis(data=df, x='LandSlope',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Null values: {df['Neighborhood'].isna().sum()}\")\n",
    "plt.figure(figsize = (30,7))\n",
    "df_copy\n",
    "\n",
    "sb.histplot(x = df['Neighborhood'], multiple='stack', hue = df.SalePrice,  data = df['Neighborhood'], hue_order = hue_order, palette = palette, alpha=1)\n",
    "df_copy = aggregate(df = df_copy, attribute='Neighborhood', new_name=\"Other\", threshold=20)\n",
    "category_common_analysis(data=df_copy, x='Neighborhood',target='SalePrice', rotation=30, palette=palette, figsize=(30,7), histplot=False)\n",
    "understand(dataf=df_copy, feature_name='Neighborhood',showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values for this attribute are strongly unbalanced among the categories inside the training set.\n",
    "#As we can see from the barplot with percentages there is not a marked trend.\n",
    "category_common_analysis(data=df, x='Condition1',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Condition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same reasoning as for Condition1\n",
    "category_common_analysis(data=df, x='Condition2',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BldgType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_common_analysis(data=df, x='BldgType',target='SalePrice', palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HouseStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see from the plots here there is a significant correlation between the style of the house and the price.\n",
    "#In particular, as the number of floors increases the SalePrice also increases\n",
    "# category_common_analysis(data=df, x='HouseStyle',target='SalePrice', palette=palette, figsize=(30,7))\n",
    "understand(dataf=df_copy, feature_name='HouseStyle')\n",
    "df_copy[\"HouseStyle\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OverallQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case the trend is evident: the higher the overall quality of the house the higher the sale price\n",
    "print(df['OverallQual'].value_counts())\n",
    "frequencies_and_percentages(df, 'SalePrice', 'OverallQual')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OverallCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this case the trend is not so marked and probably is conditioned by the fact that\n",
    "#of the houses are spread across only three of the nine possible values for this attribute\n",
    "frequencies_and_percentages(df, 'SalePrice', 'OverallCond')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the older the house the lower the associated SalePrice.\n",
    "#Moreover, for this attribute we do not have higly skewed or non uniform distribution like we had for several other attributes\n",
    "sb.boxplot(data=df, y=\"YearBuilt\", x=\"SalePrice\")\n",
    "frequencies_and_percentages_numeric(df, 'SalePrice', 'YearBuilt', rotation=30, bin=10)\n",
    "understand(dataf=df_copy, feature_name='YearBuilt', dtype='int', low_limit=1900, upper_limit=2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YearRemodAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequencies_and_percentages_numeric(df, 'SalePrice', 'YearRemodAdd', rotation=30, bin = [0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "understand(dataf=df_copy, feature_name='YearRemodAdd', dtype='int', low_limit=1900, upper_limit=2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RoofStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(x = df['RoofStyle'], multiple='stack', hue = df.SalePrice,  data = df['RoofStyle'], hue_order = hue_order, palette = palette, alpha=1)\n",
    "category_common_analysis(data=aggregate(df = df.copy(), attribute='RoofStyle', new_name=\"Other\", threshold=20), x='RoofStyle', histplot=False,target='SalePrice', palette=palette)\n",
    "understand(dataf=df_copy, feature_name='RoofStyle', dtype='category', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RoofMatl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not useful\n",
    "category_common_analysis(data=df, x='RoofMatl',target='SalePrice', percentages=False, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='RoofMatl', dtype='category', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exterior1st & Exterior2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb.histplot(x = df[\"Exterior1st\"], multiple='stack', hue = 'SalePrice',  data = df, hue_order = hue_order, palette = palette, alpha=1, ax=ax[0])\n",
    "# sb.histplot(x = df[\"Exterior2nd\"], multiple='stack', hue = 'SalePrice',  data = df, hue_order = hue_order, palette = palette, alpha=1, ax=ax[1])\n",
    "# ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "# ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "#Basically they are too similar to each other\n",
    "df_copy = aggregate(df_copy, new_name=\"Other\", threshold=40, attribute=\"Exterior1st\")\n",
    "df_copy = aggregate(df=df_copy , new_name=\"Other\", threshold=40, attribute=\"Exterior2nd\")\n",
    "plt.figure(figsize = (15,7))\n",
    "df_heatmap =  df_copy[['Exterior1st','Exterior2nd']].pivot_table(index='Exterior1st',columns='Exterior2nd',aggfunc=len)\n",
    "sb.heatmap(df_heatmap,annot=True)\n",
    "plt.show()\n",
    "category_common_analysis(data=df_copy, x='Exterior1st',target='SalePrice', histplot=False, palette=palette,rotation=45, figsize=(15, 5))\n",
    "category_common_analysis(data=df_copy, x='Exterior2nd',target='SalePrice', histplot=False, palette=palette,rotation=45, figsize=(15, 5))\n",
    "stattest_quali(df=df_copy, target='Exterior2nd', feature_name='Exterior1st')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MasVnrType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Na are only 8 elements and they correspond to unknown\n",
    "# Stone and None are significant respectively to High and Low\n",
    "#We suggest to change the 8 elements with None, since they are very few\n",
    "category_common_analysis(data=df, x='MasVnrType',target='SalePrice', percentages=True, palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MasVnrArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 8 NA values of MasVnrType have no Mas, then also this is Na, we can insert 0 (without changing the original data frame)\n",
    "#by default in order to understand better the data\n",
    "#We thing that the interaction between MasVnrType and MasVnrArea could be relevant\n",
    "#We change the type to numeric\n",
    "print(df_copy[df_copy[\"MasVnrArea\"]  == \"NA\"][[\"Id\",\"MasVnrArea\",\"MasVnrType\"]])\n",
    "for i,e in df_copy.iterrows():\n",
    "    if e[\"MasVnrArea\"]==\"NA\":\n",
    "        df_copy.at[i, \"MasVnrArea\"] = 0\n",
    "print(df_copy[df_copy[\"MasVnrArea\"]  == \"NA\"][[\"Id\",\"MasVnrArea\",\"MasVnrType\"]])\n",
    "df_copy[\"MasVnrArea\"] = df_copy[\"MasVnrArea\"].astype(\"int64\")\n",
    "print(df_copy[df_copy[\"MasVnrArea\"]  < 0][[\"Id\",\"MasVnrArea\",\"MasVnrType\"]])\n",
    "sb.boxplot(data=df_copy, y=\"MasVnrArea\", x=\"SalePrice\")\n",
    "frequencies_and_percentages_numeric(df_copy, 'SalePrice', 'MasVnrArea', rotation=30, bin=[0, 0.6, 0.8, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ExterQual & ExterCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_common_analysis(data=df, x='ExterQual',target='SalePrice', histplot=False, palette=palette)\n",
    "category_common_analysis(data=df, x='ExterCond',target='SalePrice', histplot=False, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='ExterQual', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert other for wood, slab and stone\n",
    "category_common_analysis(data=df, x='Foundation',target='SalePrice', percentages=True, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='Foundation', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert other for wood, slab and stone\n",
    "category_common_analysis(data=df, x='BsmtQual',target='SalePrice', percentages=True, palette=palette)\n",
    "df[ (df[\"SalePrice\"]==\"HIGH\")][\"BsmtQual\"].value_counts()\n",
    "understand(dataf=df_copy, feature_name='BsmtQual', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not useful, one element gather all the informations, too few elements for the other classes\n",
    "category_common_analysis(data=df, x='BsmtCond',target='SalePrice', percentages=False, palette=palette)\n",
    "df[ (df[\"SalePrice\"]==\"HIGH\")][\"BsmtCond\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtExposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[[\"BsmtExposure\",\"SalePrice\"]].pivot_table(index = \"BsmtExposure\" , columns=[\"SalePrice\"],  aggfunc=len, fill_value=0))\n",
    "category_common_analysis(data=df,x='BsmtExposure', display= False, target='SalePrice', percentages=True, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='BsmtExposure', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtFinType1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_common_analysis(data=df,x='BsmtFinType1', display= False, target='SalePrice', percentages=True, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='BsmtFinType1', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtFinSF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name='BsmtFinSF1', dtype='int', upper_limit=2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtFinType2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_common_analysis(data=df,x='BsmtFinType2', display= False, target='SalePrice', percentages=False, palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtFinSF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see from the boxplot all the means are approximately zero and also within the narrow range of plausible values\n",
    "# no trend emerges\n",
    "sb.boxplot(data=df, y=\"BsmtFinSF2\", x=\"SalePrice\")\n",
    "histplot(df,'SalePrice',\"BsmtFinSF2\",multiple='dodge',alpha=0.5, bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtUnfSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean of the attribute is approximately equal for the MEDIUM and HIGH class, while for LOW it is smaller.\n",
    "# However, the distributions of the three lables have almost the same peak\n",
    "sb.boxplot(data=df, y=\"BsmtUnfSF\", x=\"SalePrice\")\n",
    "histplot(df,'SalePrice',\"BsmtUnfSF\",multiple='layer',alpha=0.5, bins = 40)\n",
    "#frequencies_and_percentages_numeric(df, 'SalePrice', 'BsmtUnfSF', rotation=30, bin = [0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TotalBsmtSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean is different for all the three labels and the distributions of the labels\n",
    "# look enough distant one from the other, so we keep the attribute\n",
    "sb.boxplot(data=df, y=\"TotalBsmtSF\", x=\"SalePrice\")\n",
    "histplot(df,'SalePrice',\"TotalBsmtSF\",multiple='dodge',alpha=0.5, bins = 40)\n",
    "understand(dataf=df_copy, feature_name='TotalBsmtSF', dtype='int', showplots=False, low_limit=0, upper_limit= 3000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost all the examples have the same attribute value, there is no use in keeping it \n",
    "category_common_analysis(data=df,x='Heating', display= False, target='SalePrice', percentages=False, palette=palette)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HeatingQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_common_analysis(data=df,x='HeatingQC', display= False, target='SalePrice', percentages=True, palette=palette)\n",
    "understand(dataf=df_copy, feature_name='HeatingQC', showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CentralAir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG(df,'SalePrice','CentralAir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_and_percentages(df,'SalePrice','CentralAir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stattest_quali(df,'CentralAir', typeplot='heatmap', fig_width=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stattest_quanti(df,'YearBuilt', 'CentralAir' , low_limit=1900, upper_limit=2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_and_percentages(df,'SalePrice','Electrical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG(df,'SalePrice','Electrical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Electrical'] == 'NA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1stFlrSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df,'SalePrice','1stFlrSF',bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stattest_quanti(df,'1stFlrSF',low_limit=10, upper_limit=4000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2ndFlrSF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df,'SalePrice','2ndFlrSF',bins=10)\n",
    "stattest_quanti(df,'2ndFlrSF',low_limit=10, upper_limit=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LowQualFinSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df,'SalePrice','LowQualFinSF',bins=10)\n",
    "stattest_quanti(df,'LowQualFinSF',low_limit=0, upper_limit=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df,'SalePrice','GrLivArea',bins=10)\n",
    "stattest_quanti(df,'GrLivArea',low_limit=0, upper_limit=3000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtFullBath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'BsmtFullBath')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BsmtHalfBath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'BsmtHalfBath')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FullBath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'FullBath')\n",
    "IG(df, TARGET_FEATURE, 'FullBath')\n",
    "understand(df_copy, feature_name=\"FullBath\", showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HalfBath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'HalfBath')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BedroomAbvGr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'BedroomAbvGr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KitchenAbvGr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'KitchenAbvGr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KitchenQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_and_percentages(df, TARGET_FEATURE, 'KitchenQual')\n",
    "IG(df, TARGET_FEATURE, 'KitchenQual')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TotRmsAbvGrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_and_percentages(df, TARGET_FEATURE, 'TotRmsAbvGrd')\n",
    "IG(df, TARGET_FEATURE, 'TotRmsAbvGrd')\n",
    "stattest_quanti(df, 'TotRmsAbvGrd', upper_limit=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['Functional'] = df_copy['Functional'].astype('category')\n",
    "#df_copy['Functional'].unique()\n",
    "frequencies_and_percentages(df_copy, TARGET_FEATURE, 'Functional')\n",
    "#stattest_quali(df_copy, 'Functional')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fireplaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_ordinal_plot(df_copy, 'Fireplaces', test_typeplot='barplot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FireplaceQu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['FireplaceQu'] = df_copy['FireplaceQu'].astype('category')\n",
    "frequencies_and_percentages(df_copy, TARGET_FEATURE, 'FireplaceQu')\n",
    "stattest_quali(df_copy, 'FireplaceQu')\n",
    "IG(df_copy, TARGET_FEATURE, 'FireplaceQu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GarageType'].unique()\n",
    "frequencies_and_percentages(df, TARGET_FEATURE, 'GarageType')\n",
    "IG(df, TARGET_FEATURE, 'GarageType')\n",
    "understand(dataf=df_copy, feature_name=\"GarageType\", showplots=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageYrBlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy2 = df_copy.drop(df_copy[df_copy['GarageYrBlt'] == 'NA'].index)\n",
    "df_copy2['GarageYrBlt'] = df_copy2['GarageYrBlt'].astype(\"int64\")\n",
    "\n",
    "histplot(df_copy2, TARGET_FEATURE, 'GarageYrBlt', figsize=(20, 5), xticks_rot=45, bins=50)\n",
    "correlation(df_copy2, 'GarageYrBlt', 'YearBuilt')\n",
    "correlation(df_copy2, 'GarageYrBlt', TARGET_FEATURE_CONTI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['GarageFinish'] = df['GarageFinish']\n",
    "print(df_copy['GarageFinish'].unique())\n",
    "df_copy['GarageFinish'] = df_copy['GarageFinish'].astype('category')\n",
    "frequencies_and_percentages(df_copy, TARGET_FEATURE, 'GarageFinish')\n",
    "stattest_quali(df_copy, 'GarageFinish')\n",
    "IG(df_copy, TARGET_FEATURE, 'GarageFinish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'GarageFinish', 'category', showplots=True, na = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageCars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand(df_copy, 'GarageCars', 'category', showplots=True)\n",
    "categoric_ordinal_plot(df_copy, 'GarageCars')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df_copy,TARGET_FEATURE ,'GarageArea', bins=200)\n",
    "# correlation(df_copy,'GarageCars','GarageArea')\n",
    "stattest_quanti(df_copy, 'GarageArea', 'GarageCars', low_limit= 100, upper_limit= 1500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageQual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy,'GarageQual',dtype='category',showplots=True,na=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GarageCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy,'GarageCond')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PavedDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy,'PavedDrive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WoodDeckSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'WoodDeckSF', dtype='int', bins=100, upper_limit=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenPorchSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'OpenPorchSF', dtype='int', bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EnclosedPorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'EnclosedPorch', dtype='int', bins=1000, xlim=(0,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3SsnPorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, '3SsnPorch', dtype='int', bins=1000, xlim=(0,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ScreenPorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'ScreenPorch', dtype='int', bins=1500, xlim=(0,3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PoolArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'PoolArea', dtype='int', bins=1500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PoolQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name= 'PoolQC', na = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name= 'Fence', na = 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MiscFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name= 'MiscFeature', na = 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MiscVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'MiscVal', dtype='int', bins=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MoSold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name='MoSold', dtype='object', rotation=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YrSold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(df_copy, 'YrSold', dtype='int', upper_limit=2023, low_limit= 2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SaleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name='SaleType')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SaleCondition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understand(dataf=df_copy, feature_name='SaleCondition')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TotalSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[\"TotalSF\"] = total_square_feet(df_copy)\n",
    "\n",
    "understand(dataf=df_copy, feature_name='TotalSF', dtype='int', upper_limit=8000)\n",
    "frequencies_and_percentages_numeric(df=df_copy, target=TARGET_FEATURE, attribute='TotalSF', bin=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], rotation=45)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good predictive features after data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_FEATURES = ['MSSubClass', 'LotArea', 'Neighborhood', 'HouseStyle', 'OverallQual',\n",
    "                'YearBuilt', 'YearRemodAdd', 'Exterior1st', 'MasVnrType', 'MasVnrArea',\n",
    "                \"ExterQual\", \"Foundation\", \"BsmtQual\", \"BsmtFinType1\", \"TotalBsmtSF\",\n",
    "                \"1stFlrSF\", \"2ndFlrSF\", \"KitchenQual\", \"GrLivArea\", \"FullBath\", \"TotRmsAbvGrd\",\n",
    "                \"Fireplaces\", \"GarageType\", \"GarageFinish\", \"GarageArea\", 'TotalSF']\n",
    "GOOD_FEATURES.append(TARGET_FEATURE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between good predictive features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand all correlations between good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_clique(graph: list[list[int]] , set_nodes : list[int] ):\n",
    "\n",
    "    for i in set_nodes:\n",
    "        for j in set_nodes:\n",
    "            if i != j and graph[i][j] == 0:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def decide_maximal_clique(graph: list[list] , set_nodes : list[int]):\n",
    "    \n",
    "    if not decide_clique(graph=graph, set_nodes=set_nodes):\n",
    "        return False\n",
    "    \n",
    "    for i in range(len(graph)):\n",
    "        if i not in set_nodes:\n",
    "            b = True\n",
    "            for j in set_nodes:\n",
    "                if graph[i][j] == 0:\n",
    "                    b = False\n",
    "            if b :\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def function_maximal_cliques(graph: list[list], n: int) -> list[list[int]]:\n",
    "    \n",
    "    if(len(graph) < n or n <= 0):\n",
    "        return None \n",
    "\n",
    "    res = []\n",
    "    \n",
    "    maximal_clique = [ i for i in range(n)]\n",
    "\n",
    "    i = n - 1\n",
    "    while( maximal_clique[0] < len(graph) - n + 1):\n",
    "\n",
    "        if i == n - 1 and decide_maximal_clique(graph=graph, set_nodes=maximal_clique):\n",
    "             res.append(maximal_clique.copy())\n",
    "        \n",
    "        maximal_clique[i] += 1\n",
    "        if maximal_clique[i] < (len(graph) - n + 1 + i):\n",
    "            # Can increment all counters up to n - 1\n",
    "            for j in range(i+1,n):\n",
    "                maximal_clique[j] = maximal_clique[j-1] + 1\n",
    "            i = n - 1 \n",
    "        else:\n",
    "            i -= 1   \n",
    "        \n",
    "    return res\n",
    "        \n",
    "\n",
    "def function_maximal_cliques_all(graph: list[list]) -> list[list[int]]:\n",
    "    \n",
    "    n = 5\n",
    "\n",
    "    res = []\n",
    "    while n > 0 :\n",
    "        resi = function_maximal_cliques(graph=graph, n=n)\n",
    "\n",
    "        n -= 1\n",
    "        if(resi != []):\n",
    "            res.append(resi)\n",
    "\n",
    "    return res\n",
    "        \n",
    "\n",
    "\n",
    "def understand_correlation_all(dataf : pd.DataFrame, selected: list, threshold = 0.65, cliques = False):\n",
    "    \n",
    "    correlation_scores = {}\n",
    "\n",
    "\n",
    "    graph = [ [ 0 for _ in range(len(selected)) ] for _ in range(len(selected))  ]\n",
    "    corr_graph = graph.copy()\n",
    "\n",
    "    for i in range(len(selected)):\n",
    "        for j in range(i+1, len(selected)):\n",
    "            corr = 0\n",
    "            bcategoryi = is_categorical_dtype(dataf[selected[i]].dtype) or dataf[selected[i]].dtype == 'object'\n",
    "            bcategoryj = is_categorical_dtype(dataf[selected[j]].dtype) or dataf[selected[j]].dtype == 'object'\n",
    "            binti = is_integer_dtype(dataf[selected[i]].dtype)\n",
    "            bintj = is_integer_dtype(dataf[selected[j]].dtype)\n",
    "\n",
    "            if (bcategoryi or bcategoryj) and \\\n",
    "                (binti  or bintj  ):\n",
    "                if bcategoryi:\n",
    "                    _, _, corr  = stattest_quanti(dataf,feature_name=selected[j], target=selected[i], silent=True, cramer=True )\n",
    "                else:\n",
    "                    _, _, corr  = stattest_quanti(dataf,feature_name=selected[i], target=selected[j], silent=True, cramer=True )\n",
    "            elif bcategoryi and bcategoryj:\n",
    "                _, _, corr  = stattest_quali(dataf,feature_name=selected[j], target=selected[i], silent=True, cramer=True )\n",
    "            elif binti and bintj: \n",
    "                corr, _  = correlation(dataf, n1 = selected[i], n2 = selected[j], silent=True)\n",
    "            else:\n",
    "                if not bcategoryi and  not binti:\n",
    "                    print(\"ERROR -> \"+selected[i]+ \" type -> \"+ str(dataf[selected[i]].dtype))\n",
    "                if not bcategoryj and not bintj:\n",
    "                    print(\"ERROR -> \"+selected[j]+ \" type -> \"+ str(dataf[selected[j]].dtype))\n",
    "            \n",
    "            corr = abs(corr)\n",
    "            if  corr >= threshold:\n",
    "                graph[i][j] = 1\n",
    "                graph[j][i] = 1\n",
    "            corr_graph[i][j] = corr\n",
    "            corr_graph[j][i] = corr\n",
    "            correlation_scores[selected[i] + \"<->\" + selected[j]] = corr\n",
    "\n",
    "    for fi in range(len(corr_graph)):\n",
    "        corr_graph[fi][fi] = 1.0\n",
    "\n",
    "    if cliques:\n",
    "        for n in function_maximal_cliques_all(graph=graph):\n",
    "            ni = len(n[0])\n",
    "            print(\"cliques of size: \"+str(ni))\n",
    "            for clique in n:\n",
    "                print(\"{\", end=\" \")\n",
    "                for i in clique:\n",
    "                    print(selected[i], end=\" \")\n",
    "                print(\"}\")\n",
    "    return corr_graph\n",
    "\n",
    "\n",
    "#\n",
    "# corr matrix/graph\n",
    "#\n",
    "corr_graph = understand_correlation_all(df_copy, selected=GOOD_FEATURES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature correlation selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_correlation_selection(corr_graph_full: list, threshold: float=0.5, iters: int=1, show_heatmap: bool=True):  # threashold must be > 0.0\n",
    "    import copy\n",
    "    corr_graph = copy.deepcopy(corr_graph_full)\n",
    "\n",
    "    nnodes = len(corr_graph)\n",
    "    target_fidx = GOOD_FEATURES.index(TARGET_FEATURE)\n",
    "    \n",
    "    import networkx as nx\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(nnodes))\n",
    "\n",
    "    keep_history_nodes = []\n",
    "    \n",
    "    for iter_idx in range(iters):\n",
    "        print(\"Iteration %d\" % (iter_idx + 1))\n",
    "        print('-' * 20)\n",
    "\n",
    "        ignored_nodes = []\n",
    "        while True:\n",
    "            max_fi = -1\n",
    "            max_fj = -1\n",
    "            max_corr = 0\n",
    "            for fi in range(nnodes):\n",
    "                for fj in range(fi+1, nnodes):\n",
    "                    if fi == target_fidx or fj == target_fidx or (fi in ignored_nodes and fj in ignored_nodes):\n",
    "                        continue\n",
    "                    if corr_graph[fi][fj] > max_corr:\n",
    "                        max_fi = fi\n",
    "                        max_fj = fj\n",
    "                        max_corr = corr_graph[fi][fj]\n",
    "\n",
    "            if max_corr < threshold:\n",
    "                break\n",
    "            \n",
    "            #print(max_corr)\n",
    "            to_keep = max_fi\n",
    "            to_remove = max_fj\n",
    "            if corr_graph[max_fj][target_fidx] > corr_graph[max_fi][target_fidx]:\n",
    "                to_keep = max_fj\n",
    "                to_remove = max_fi\n",
    "            \n",
    "            #print(\"CORR:\", corr_graph[to_keep][target_fidx], corr_graph[to_remove][target_fidx])\n",
    "\n",
    "            ignored_nodes.append(to_keep)\n",
    "            keep_history_nodes.append(to_keep)\n",
    "\n",
    "            to_keep_name = GOOD_FEATURES[to_keep]\n",
    "            to_remove_name = GOOD_FEATURES[to_remove]\n",
    "\n",
    "            if to_remove in ignored_nodes:\n",
    "                G.add_edge(to_keep, to_remove, weight=max_corr)\n",
    "                G.add_edge(to_remove, to_keep)\n",
    "                print(\"Keep %s (cannot remove %s)\" % (to_keep_name, to_remove_name))\n",
    "            else:\n",
    "                for k in range(nnodes):\n",
    "                    corr_graph[to_remove][k] = 0.0\n",
    "                    corr_graph[k][to_remove] = 0.0\n",
    "                ignored_nodes.append(to_remove)\n",
    "                G.add_edge(to_keep, to_remove, weight=max_corr)\n",
    "\n",
    "                if to_remove in keep_history_nodes: print(\"Keep %s (cannot remove %s)\" % (to_keep_name, to_remove_name))\n",
    "                else: print(\"Keep %s and remove %s\" % (to_keep_name, to_remove_name))\n",
    "        print()\n",
    "\n",
    "    corr_graph = corr_graph_full\n",
    "\n",
    "    def remap_fullrange(value: float, threshold: float, new_lower=0.0, new_upper=1.0):\n",
    "        return new_lower + (value - threshold) * (new_upper - new_lower) / (new_upper - threshold)\n",
    "\n",
    "    #\n",
    "    # plot corr heatmap\n",
    "    #\n",
    "    if show_heatmap:\n",
    "        fig_size = min(20, 0.55 * nnodes)\n",
    "        fig, ax = plt.subplots(figsize=(fig_size, fig_size))  \n",
    "        corr_graph_df = pd.DataFrame(corr_graph, index=GOOD_FEATURES, columns=GOOD_FEATURES)\n",
    "        sb.heatmap(corr_graph_df, annot=True, cmap='Blues', fmt='.2f', vmin=threshold, cbar_kws={\"shrink\": 0.5}, ax=ax)\n",
    "        ax.set_title('Selected Features')    \n",
    "        ax.set_title('Selected Features')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #\n",
    "    # draw corr graph (G)\n",
    "    #\n",
    "\n",
    "    fig_net = plt.figure(figsize=(14, 7))\n",
    "    ax_net = plt.subplot(111)\n",
    "    ax_net.margins(0.1)  \n",
    "    ax_net.set_title('Correlation Graph')\n",
    "    #ax_net.set_facecolor(\"white\")\n",
    "\n",
    "\n",
    "    #\n",
    "    # graph layout\n",
    "    #\n",
    "    gdf = pd.DataFrame(index=G.nodes(), columns=G.nodes())\n",
    "    for row, data in nx.shortest_path_length(G):\n",
    "        for col, dist in data.items():\n",
    "            gdf.loc[row,col] = dist\n",
    "\n",
    "    gdf = gdf.fillna(gdf.max().max())\n",
    "    pos = nx.kamada_kawai_layout(G, dist=gdf.to_dict())\n",
    "\n",
    "    #\n",
    "    # draw nodes\n",
    "    #\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    node_labels_dict = {}\n",
    "    for fi in range(nnodes):\n",
    "        node_sizes.append(5000 * remap_fullrange(corr_graph[fi][target_fidx], threshold, new_lower=0.3))\n",
    "        node_colors.append('cadetblue' if G.out_degree(fi) > 0 or G.in_degree(fi) == 0 else 'lightgray')\n",
    "        node_labels_dict[fi] = GOOD_FEATURES[fi]\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, ax=ax_net)\n",
    "\n",
    "    #\n",
    "    # draw edge weight labels\n",
    "    #\n",
    "    #edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "    #for k in edge_labels.keys():\n",
    "    #    edge_labels[k] = float(f'{edge_labels[k]:.2f}')\n",
    "    #nx.draw_networkx_edge_labels(G, pos, edge_labels, verticalalignment='bottom')\n",
    "\n",
    "    #\n",
    "    # draw edges\n",
    "    #\n",
    "    edges_list, edge_weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    edge_weights = list(edge_weights)\n",
    "    for i in range(len(edge_weights)):\n",
    "        edge_weights[i] = remap_fullrange(edge_weights[i], threshold)\n",
    "    edge_weights = tuple(edge_weights)\n",
    "\n",
    "    import matplotlib.colors as colors\n",
    "    def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "        new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "            'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "            cmap(np.linspace(minval, maxval, n)))\n",
    "        return new_cmap\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    edge_cmap = truncate_colormap(plt.cm.Blues, 0.3, 1.0)\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=0.5, vmax=1.0)\n",
    "    fig_net.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=edge_cmap), label='Correlation Coefficient')\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=edges_list, edge_color=edge_weights, width=3, edge_cmap=edge_cmap, arrowsize=20, arrowstyle='-', ax=ax_net)\n",
    "\n",
    "    #\n",
    "    # draw node labels\n",
    "    #\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"sans-serif\", font_weight='bold', labels=node_labels_dict, ax=ax_net)\n",
    "\n",
    "\n",
    "feature_correlation_selection(corr_graph, iters=10, show_heatmap=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for inconsistencies among good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconsistencies(df):\n",
    "    print(\"Incons. for Garage:\")\n",
    "    print(f'If no garage, then no garageFinish {len(df[(df[\"GarageType\"] == \"NA\") & (df[\"GarageFinish\"] != \"NA\")])}')\n",
    "    print(f'If no garage, then no garageArea {len(df[(df[\"GarageType\"] == \"NA\") & (df[\"GarageArea\"] != 0)])}')\n",
    "    print()\n",
    "    print(\"Incons. for Basment:\")\n",
    "    print(f'BsmtFinType1 <=> bsmtQual {len(df[((df[\"BsmtQual\"] == \"NA\") & (df[\"BsmtFinType1\"] != \"NA\")) | ((df[\"BsmtFinType1\"] == \"NA\") & (df[\"BsmtQual\"] != \"NA\"))])}')\n",
    "    print(f'TotalBsmtSf <=> bsmtQual {len(df[((df[\"BsmtQual\"] == \"NA\") & (df[\"TotalBsmtSF\"] != 0)) | ((df[\"TotalBsmtSF\"] == 0) & (df[\"BsmtQual\"] != \"NA\"))])}')\n",
    "    print(f'BsmtFinType1 <=> TotalBsmtSF {len(df[((df[\"BsmtFinType1\"] == \"NA\") & (df[\"TotalBsmtSF\"] != 0)) | ((df[\"TotalBsmtSF\"] == 0) & (df[\"BsmtFinType1\"] != \"NA\"))])}')\n",
    "    print()\n",
    "    print(\"Incons. for floors:\")\n",
    "    print(f'Sum of 1st and 2nd floor is consistent: {len(df[df[\"GrLivArea\"] < df[\"1stFlrSF\"]+df[\"2ndFlrSF\"]])}')\n",
    "    print(f'1st floor <= 0: {len(df[df[\"1stFlrSF\"] <= 0])}')\n",
    "    print()\n",
    "    print(\"Incons. for Housestyle:\")\n",
    "    house_style_df = df[(df[\"HouseStyle\"] == \"1Story\") & (df[\"2ndFlrSF\"] > 0 )]\n",
    "    print(f'{house_style_df.loc[:, [\"Id\",\"HouseStyle\",\"2ndFlrSF\", \"1stFlrSF\",\"salePriceNum\", \"SalePrice\",\"GrLivArea\"]]}')\n",
    "    print()\n",
    "    print(\"Incons. for Rooms:\")\n",
    "    print(f'At least a room: {len(df[(df[\"TotRmsAbvGrd\"] == 0)])}')\n",
    "    print()\n",
    "    print(\"Incons. for MasVnrType:\")\n",
    "    df_mas = df.loc[:,[\"Id\", \"MasVnrType\", \"MasVnrArea\", \"SalePrice\"]]\n",
    "    df_mas[\"MasVnrArea\"] = df_mas[\"MasVnrArea\"].replace(\"NA\", 0)\n",
    "    df_mas[\"MasVnrArea\"] = df_mas[\"MasVnrArea\"].astype(\"int\")\n",
    "    #3 diversi vanno droppati e i due con 1 in MasVnrType vanno settati a 0 \n",
    "    print(f'No MasVnr then Area must be 0: {(df_mas[(df_mas[\"MasVnrType\"] == \"None\") & (df_mas[\"MasVnrArea\"] != 0)])}')\n",
    "    #to be changed to None!\n",
    "    print(f'NA MasVnr: {(df_mas[(df_mas[\"MasVnrType\"] == \"NA\") & (df_mas[\"MasVnrArea\"] != 0)])}')\n",
    "    print()\n",
    "    for i in df.columns:\n",
    "        if is_integer_dtype(df[i].dtype):\n",
    "            if(len(df[df[i]<0]) > 0):\n",
    "                print(f\"Values < 0 for {i}\")\n",
    "\n",
    "\n",
    "inconsistencies(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleaning and Reduction of good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_FEATURES_TYPE_MAP = \\\n",
    "    {\"MSSubClass\": [\"ordinal integer\"],\n",
    "    \"Neighborhood\": [\"category\"],\n",
    "    \"HouseStyle\": [\"category\"],\n",
    "    \"OverallQual\": [\"int64\"],\n",
    "    \"YearBuilt\": [\"int64\"],\n",
    "    \"YearRemodAdd\": [\"int64\"],\n",
    "    \"Exterior1st\":[\"category\"],\n",
    "    \"MasVnrType\":[\"category\"],\n",
    "    \"MasVnrArea\":[\"int64\"],\n",
    "    \"ExterQual\":[\"ordinal category\", [\"Po\", \"Fa\",\"TA\",\"Gd\",\"Ex\"]],\n",
    "    \"Foundation\":[\"category\"],\n",
    "    \"BsmtQual\":[\"ordinal category\", [\"NA\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"]],\n",
    "    \"BsmtFinType1\":[\"ordinal category\", [\"NA\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"]],\n",
    "    \"TotalBsmtSF\":[\"int64\"],\n",
    "    \"1stFlrSF\":[\"int64\"],\n",
    "    \"2ndFlrSF\":[\"int64\"],\n",
    "    \"GrLivArea\":[\"int64\"],\n",
    "    \"FullBath\":[\"int64\"],\n",
    "    \"KitchenQual\":[\"ordinal category\", [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"]],\n",
    "    \"TotRmsAbvGrd\":[\"int64\"],\n",
    "    \"Fireplaces\":[\"int64\"],\n",
    "    \"GarageType\":[\"category\"],\n",
    "    \"GarageFinish\":[\"ordinal category\",[\"NA\",\"Unf\",\"RFn\",\"Fin\"]],\n",
    "    \"GarageArea\":[\"int64\"],\n",
    "    \"LotArea\": ['int64'],\n",
    "    \"SalePrice\":[\"category\"]}\n",
    "\n",
    "INTEGER_ATTRIBUTES = []\n",
    "\n",
    "for key in GOOD_FEATURES_TYPE_MAP.keys():\n",
    "    if GOOD_FEATURES_TYPE_MAP[key][0] == 'int64':\n",
    "        INTEGER_ATTRIBUTES.append(key)\n",
    "INTEGER_ATTRIBUTES += ['LotFrontage', 'BsmtFinSF1', \"BsmtFinSF2\", 'BsmtUnfSF', \"LowQualFinSF\",\n",
    "                       \"BsmtFullBath\", \"BsmtHalfBath\", \"HalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\",\n",
    "                       \"GarageYrBlt\", \"GarageCars\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "                       \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\", \"MoSold\",  \"YrSold\" ]\n",
    "\n",
    "\n",
    "def data_cleaning_reduction(df: pd.DataFrame, selected_att : dict):\n",
    "    df = df.copy(deep=True)\n",
    "    #feature selection (era in data reduction, per facilita lo teniamo qua)\n",
    "    df = df.loc[:, selected_att.keys()]\n",
    "\n",
    "    #missing values there are no null values only NA\n",
    "    #df[\"attributo\"].fillna(\"valore\", inplace = True)\n",
    "\n",
    "    na_values = {\"MasVnrArea\": [\"NA\", 0], \"MasVnrType\": [\"NA\", \"None\"], \"GarageYrBlt\":[\"NA\", 0], \"2ndFlrSF\":[\"NA\", 0], \"LotFrontage\":[\"NA\", 0]}\n",
    "    for attribute in na_values.keys():\n",
    "        if attribute in df.columns:\n",
    "            df[attribute] = df[attribute].replace(na_values[attribute][0], na_values[attribute][1]) \n",
    "    \n",
    "    #casting\n",
    "    for i in selected_att.keys():\n",
    "        if i not in df.columns:\n",
    "            pass\n",
    "        if selected_att[i][0] == \"ordinal category\":\n",
    "            df[i] = df[i].astype(\"category\")\n",
    "            df[i] = df[i].cat.set_categories(selected_att[i][1], ordered=True)\n",
    "        elif selected_att[i][0] == \"ordinal integer\":\n",
    "            df[i]=df[i].astype(CategoricalDtype(ordered=True))\n",
    "        else:\n",
    "            df[i] = df[i].astype(selected_att[i][0])\n",
    "    \n",
    "    #noisy data\n",
    "    if \"HouseStyle\" in df.columns and \"2ndFlrSF\" in df.columns:\n",
    "        df.loc[(df[\"HouseStyle\"] == \"1Story\") & (df[\"2ndFlrSF\"] > 0 ), \"HouseStyle\"] = \"2Story\"\n",
    "    if \"MasVnrType\" in df.columns and \"MasVnrArea\" in df.columns:\n",
    "        df.loc[(df[\"MasVnrType\"] == \"None\") & (df[\"MasVnrArea\"] <= 5 ), \"MasVnrArea\"] = 0\n",
    "        df_mastype = df[(df[\"MasVnrType\"] == \"None\") & (df[\"MasVnrArea\"] > 5 )]\n",
    "        for index, row in df_mastype.iterrows():\n",
    "            df.at[index, \"MasVnrType\"] = get_common_category(df, category=row[\"SalePrice\"], attribute=\"MasVnrType\", exclude = [\"None\", \"NA\"])\n",
    "    return df\n",
    "    \n",
    "\n",
    "df_cleaned = data_cleaning_reduction(df, selected_att=GOOD_FEATURES_TYPE_MAP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation of good features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_FEATURES_AGGREGATED_VALUES = {\n",
    "        # \"HouseStyle\": {\n",
    "        #                 \"1Story\" : \"other\",\n",
    "        #                 \"1.5Unf\" : \"other\",\n",
    "        #                 \"2.5Fin\" : \"other\",\n",
    "        #                 \"2.5Unf\" : \"other\",\n",
    "        #                 \"Sfoyer\" : \"other\",\n",
    "        #                 \"SLvl\" : \"other\"\n",
    "        #                 },\n",
    "        \"OverallQual\": {\n",
    "                        1 : 4,\n",
    "                        2 : 4,\n",
    "                        3 : 4,\n",
    "                        10 : 9,\n",
    "                        },\n",
    "        # \"Exterior1st\": {\n",
    "        #                 \"AsbShng\" : \"other\",\n",
    "        #                 \"AsphShn\" : \"other\",\n",
    "        #                 \"BrkComm\" : \"other\",\n",
    "        #                 \"BrkFace\" : \"other\",\n",
    "        #                 \"CBlock\" : \"other\",\n",
    "        #                 \"CemntBd\" : \"other\",\n",
    "        #                 \"HdBoard\" : \"other\",\n",
    "        #                 \"ImStucc\" : \"other\",                                    \n",
    "        #                 \"AsbShng\" : \"other\",\n",
    "        #                 \"MetalSd\" : \"other\",\n",
    "        #                 \"Other\" : \"other\",\n",
    "        #                 \"Plywood\" : \"other\",\n",
    "        #                 \"PreCast\" : \"other\",\n",
    "        #                 \"Stone\" : \"other\",\n",
    "        #                 \"Stucco\" : \"other\",\n",
    "        #                 \"Stone\" : \"other\",\n",
    "        #                 \"Wd Sdng\" : \"other\",\n",
    "        #                 \"WdShing\" : \"other\"\n",
    "\n",
    "        #                 },\n",
    "        # \"MasVnrType\": {\n",
    "        #                 \"BrkCmn\" : \"other\",\n",
    "        #                 \"BrkFace\" : \"other\"\n",
    "        #                 },\n",
    "        # \"ExterQual\": {\n",
    "        #                 \"Fa\" : \"TA\",\n",
    "        #                 \"Po\" : \"TA\",\n",
    "        #             },\n",
    "        \"Foundation\": {\n",
    "                        \"wood\" : \"other\",\n",
    "                        \"slab\" : \"other\",\n",
    "                        \"stone\" : \"other\",\n",
    "                        \"BrkTil\" : \"CBlock\"\n",
    "                    },\n",
    "        # \"BsmtQual\": {\n",
    "        #                 \"NA\" : \"other\",\n",
    "        #                 \"Fa\" : \"other\",\n",
    "        #                 \"TA\" : \"other\",\n",
    "        #                 \"Po\" : \"other\"\n",
    "        #             },\n",
    "        # \"BsmtFinType1\": {\n",
    "        #                 \"ALQ\" : \"other\",\n",
    "        #                 \"BLQ\" : \"other\",\n",
    "        #                 \"Rec\" : \"other\",\n",
    "        #                 \"LwQ\" : \"other\",\n",
    "        #                 \"NA\" : \"other\",\n",
    "        #             },\n",
    "        # \"KitchenQual\": {\n",
    "        #                 \"Fa\" : \"TA\",\n",
    "        #                 \"Po\" : \"TA\",\n",
    "        #             },\n",
    "        # \"GarageType\": {\n",
    "        #                 \"2Types\" : \"other\",\n",
    "        #                 \"CarPort\" : \"other\",\n",
    "        #                 \"Basment\" : \"other\",\n",
    "        #             },\n",
    "        }\n",
    "\n",
    "\n",
    "def data_transformation(df : pd.DataFrame, aggregate_values, scale = True, aggr=True, fcr=True):\n",
    "    df = df.copy(deep=True)\n",
    "    #Normalization\n",
    "    if scale:\n",
    "        type = [\"float64\", \"int\" ,\"int64\", \"float\"]\n",
    "        cols = df.select_dtypes(include = type).columns\n",
    "        scaler = StandardScaler()\n",
    "        df[cols] = scaler.fit_transform(df[cols])\n",
    "    #Aggregation\n",
    "    if aggr:\n",
    "        for attribute in aggregate_values.keys():\n",
    "            for val in aggregate_values[attribute].keys():\n",
    "                df[attribute] = df[attribute].replace(val, aggregate_values[attribute][val])\n",
    "\n",
    "    #Feature creation\n",
    "    if fcr:\n",
    "        df[\"TotalSF\"] = total_square_feet(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_transformed = data_transformation(df_cleaned, GOOD_FEATURES_AGGREGATED_VALUES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection with wrapper approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping_evaluation(df : pd.DataFrame, model, n_iterations, silent: bool=False, metric = 'accuracy'):\n",
    "    n_size = df.shape[0] // 2\n",
    "    values = df.values\n",
    "    #Lets run Bootstrap\n",
    "    stats = list()\n",
    "    for i in range(n_iterations):\n",
    "\n",
    "        #prepare train & test sets\n",
    "        train = resample(df.index, n_samples = n_size) #Sampling with replacement..whichever is not used in training data will be used in test data\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        attributes = df.columns.values.tolist()\n",
    "        attributes.remove('SalePrice')\n",
    "        for ind in train:\n",
    "            row = df.iloc[ind]\n",
    "            train_y.append(row['SalePrice'])\n",
    "            row = row[attributes]\n",
    "            train_x.append(row.values)\n",
    "        \n",
    "        test = np.array([x for x in df.index if x not in train]) #picking rest of the data not considered in training sample\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        for ind in test:\n",
    "            row = df.iloc[ind]\n",
    "            test_y.append(row['SalePrice'])\n",
    "            row = row[attributes]\n",
    "            test_x.append(row.values)\n",
    "        #fit model\n",
    "        model.fit(train_x, train_y) #model.fit(X_train,y_train) i.e model.fit(train set, train label as it is a classifier)\n",
    "        \n",
    "        #evaluate model\n",
    "        predictions = model.predict(test_x) #model.predict(X_test)\n",
    "        if(metric == 'accuracy'):\n",
    "            score = accuracy_score(test_y, predictions) #accuracy_score(y_test, y_pred)\n",
    "        elif metric == 'balanced accuracy':\n",
    "            score = balanced_accuracy_score(test_y, predictions)\n",
    "            \n",
    "        #caution, overall accuracy score can mislead when classes are imbalanced\n",
    "        \n",
    "        if not silent: print(score)\n",
    "        stats.append(score)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "class DatasetScoreSampler:\n",
    "\n",
    "    def compute_score_sample(self, dataf: pd.DataFrame) -> list:\n",
    "        pass\n",
    "    def compute_score_mean(self, dataf: pd.DataFrame) -> float:\n",
    "        pass\n",
    "\n",
    "class KFoldDatasetScoreSampler(DatasetScoreSampler):\n",
    "\n",
    "    def __init__(self, k: int=5, metrics = ['balanced_accuracy'], niterations: int=20, model = ('DT', DecisionTreeClassifier())) -> None:\n",
    "        self.k = k\n",
    "        self.metrics = metrics\n",
    "        self.niterations = niterations\n",
    "        self.model_name = model[0]\n",
    "        self.model = model[1]\n",
    "    \n",
    "    def compute_score_sample(self, dataf: pd.DataFrame) -> list:\n",
    "        k_fold = KFold(shuffle=True,n_splits = self.k)\n",
    "        score_sample = []\n",
    "        \n",
    "        for _ in range(self.niterations):\n",
    "            score=0\n",
    "            for i, (train_index, test_index) in enumerate(k_fold.split(dataf)):\n",
    "                #print(f\"Fold {i}:\")\n",
    "                #print(f\"  Train: index={train_index}\")\n",
    "                #print(f\"  Test:  index={test_index}\")\n",
    "\n",
    "                train_dataf = dataf.iloc[train_index]\n",
    "                test_dataf = dataf.iloc[test_index]\n",
    "                #print(\"Size before resample: \" + str(len(train_dataf)))\n",
    "                #print(train_dataf.columns)\n",
    "                #print(train_dataf.head())\n",
    "                train_dataf_resampled = train_dataf # resample(train_dataf) TODO: fix it!\n",
    "                #print(\"Size after resample: \" + str(len(train_dataf_resampled)))\n",
    "                # print(train_dataf.columns)\n",
    "                X_train = train_dataf_resampled.drop(columns=[TARGET_FEATURE])\n",
    "                X_test  = test_dataf.drop(columns=[TARGET_FEATURE])\n",
    "                y_train = train_dataf_resampled[TARGET_FEATURE]\n",
    "                y_test  = test_dataf[TARGET_FEATURE]\n",
    "                \n",
    "\n",
    "                self.model.fit(X_train, y_train)\n",
    "                y_pred = self.model.predict(X_test)\n",
    "                \n",
    "                metric_score = 0\n",
    "                for metric in self.metrics:\n",
    "                    if metric == 'balanced_accuracy':\n",
    "                        metric_score += balanced_accuracy_score(y_true=y_test, y_pred=y_pred)  # balance accuracy as measure\n",
    "                    if metric == 'accuracy':\n",
    "                        metric_score += accuracy_score(y_true=y_test, y_pred=y_pred) # balance accuracy as measure\n",
    "                    if metric == 'f1_score':\n",
    "                        metric_score += f1_score(y_true=y_test, y_pred=y_pred) # balance accuracy as measure\n",
    "                    if metric == 'precision':\n",
    "                        metric_score += precision_score(y_true=y_test, y_pred=y_pred) # balance accuracy as measure\n",
    "                    if metric == 'recall':\n",
    "                        metric_score += recall_score(y_true=y_test, y_pred=y_pred) # balance accuracy as measure\n",
    "\n",
    "                score += metric_score / len(self.metrics)\n",
    "            score_sample.append(score / self.k)\n",
    "            \n",
    "            #score_sample.append(mean(iter_score_sample))\n",
    "        \n",
    "        return score_sample\n",
    "\n",
    "    def compute_score_mean(self, dataf: pd.DataFrame) -> float:\n",
    "        return mean(self.compute_score_sample(dataf))\n",
    "\n",
    "    def compute_score_stats(self, dataf: pd.DataFrame) -> float:\n",
    "        sample = self.compute_score_sample(dataf)\n",
    "        return mean(sample), min(sample), max(sample)\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "\n",
    "\n",
    "class FeatureSelectionValidator:\n",
    "\n",
    "    def __init__(self,global_df: pd.DataFrame, staged_dataf: pd.DataFrame,\n",
    "                 datasetScoreSampler: DatasetScoreSampler, silent: bool=False) -> None:\n",
    "        self.no_dummies_no_scale_df = staged_dataf\n",
    "        self.set_dataf(staged_dataf)\n",
    "        self.silent = silent\n",
    "        self.global_df = global_df\n",
    "        self.datasetScoreSampler = datasetScoreSampler\n",
    "        self.model_name = self.datasetScoreSampler.model_name\n",
    "        self.staged_score_sample = self.datasetScoreSampler.compute_score_sample(self.staged_dataf)\n",
    "        self.print(f\"Model name {self.model_name}\\nStaged dataframe score (avg): %f\" % mean(self.staged_score_sample), \"\\n\")\n",
    "\n",
    "    \n",
    "    def set_dataf(self, staged_dataf: pd.DataFrame, resample = False):\n",
    "        cat_attributes = list(staged_dataf.select_dtypes(include = ['category', 'object']).columns)\n",
    "        cat_attributes.remove(TARGET_FEATURE)\n",
    "        self.staged_dataf = pd.get_dummies(staged_dataf,columns=cat_attributes)\n",
    "        type = [\"float64\", \"int\" ,\"int64\", \"float\"]\n",
    "        cols_to_scale = staged_dataf.select_dtypes(include = type).columns\n",
    "        scaler = StandardScaler()\n",
    "        self.staged_dataf[cols_to_scale] = scaler.fit_transform(self.staged_dataf[cols_to_scale])\n",
    "        self.staged_dataf = pd.get_dummies(staged_dataf,columns=cat_attributes)\n",
    "        \n",
    "\n",
    "    def validate_forward(self, candidate_features: list[str]) -> list:\n",
    "        \n",
    "        include_features = []\n",
    "        candidate_features = self.compute_corr_with_target(candidate_features, order='desc') \n",
    "\n",
    "        working_staged_dataf = self.staged_dataf.copy(deep=True)\n",
    "        working_staged_score_sample = self.staged_score_sample\n",
    "\n",
    "        for candidate_feature, corr in candidate_features:\n",
    "            if candidate_feature == TARGET_FEATURE or candidate_feature == TARGET_FEATURE_CONTI: continue\n",
    "\n",
    "            # candidate data frame (construction + score sample)\n",
    "            candidate_dataf = working_staged_dataf.copy(deep=True)\n",
    "            candidate_dataf[candidate_feature] = self.global_df[candidate_feature]\n",
    "            candidate_feature_dtype = str(candidate_dataf[candidate_feature].dtype)\n",
    "            \n",
    "            # TODO: add assert for candidate_feature\n",
    "            if candidate_feature not in  INTEGER_ATTRIBUTES :\n",
    "                candidate_dataf = pd.get_dummies(candidate_dataf, columns=[candidate_feature])\n",
    "            \n",
    "            elif 'int' in candidate_feature_dtype or 'float' in candidate_feature_dtype or candidate_feature in  INTEGER_ATTRIBUTES :\n",
    "                if candidate_feature_dtype == 'object':\n",
    "                    candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].replace(to_replace='NA', value=0)\n",
    "                    candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].astype(\"int\")\n",
    "                # normalize in case of numeric dtype\n",
    "                scaler = StandardScaler()\n",
    "                candidate_dataf[[candidate_feature]] = scaler.fit_transform(candidate_dataf[[candidate_feature]])\n",
    "            \n",
    "            else: raise RuntimeError('Invalid dtype for candidate feature.')\n",
    "            \n",
    "            candidate_score_sample = self.datasetScoreSampler.compute_score_sample(candidate_dataf)\n",
    "            \n",
    "            # perform t-test for the means\n",
    "            _, pvalue = stats.ttest_ind(candidate_score_sample, working_staged_score_sample, equal_var=False, alternative='greater')\n",
    "            outcome: bool = pvalue < 0.05\n",
    "\n",
    "            if outcome:  # evidence to an increase in performance\n",
    "                include_features.append(candidate_feature)\n",
    "                working_staged_dataf = candidate_dataf\n",
    "                working_staged_score_sample = candidate_score_sample\n",
    "\n",
    "            self.print(\"%s selection - Validation outcome:\" % candidate_feature)\n",
    "            self.print(\"\\t- %s candidate\" % ('include' if outcome else 'discard'))\n",
    "            self.print(\"\\t- pvalue: %f\" % pvalue, \"\\n\")\n",
    "        \n",
    "        final_score_mean = self.datasetScoreSampler.compute_score_mean(working_staged_dataf)\n",
    "        self.print(f\"Model name {self.model_name}\\nFinal dataframe score (avg): %f\" % final_score_mean, \"\\n\")\n",
    "        # self.print(\"Final dataset cols:\", str(working_staged_dataf.columns.tolist()))\n",
    "        \n",
    "        return include_features\n",
    "\n",
    "    def validate_forward_pvalue(self, candidate_features: list[str]) -> list:\n",
    "        \n",
    "        include_features = []\n",
    "        candidate_features = candidate_features.copy()\n",
    "\n",
    "        working_staged_dataf = self.staged_dataf.copy(deep=True)\n",
    "        working_staged_score_sample = self.staged_score_sample\n",
    "\n",
    "        while len(candidate_features) > 0:\n",
    "            min_pvalue = 0.0\n",
    "            min_candidate_feature = None\n",
    "            min_candidate_score_sample = []\n",
    "\n",
    "            for candidate_feature in candidate_features:\n",
    "                if candidate_feature == TARGET_FEATURE or candidate_feature == TARGET_FEATURE_CONTI: continue\n",
    "                \n",
    "                # candidate data frame (construction + score sample)\n",
    "                candidate_dataf = working_staged_dataf.copy(deep=True)\n",
    "                candidate_dataf[candidate_feature] = self.global_df[candidate_feature]\n",
    "                candidate_feature_dtype = str(candidate_dataf[candidate_feature].dtype)\n",
    "                \n",
    "                # TODO: add assert for candidate_feature\n",
    "                if candidate_feature not in  INTEGER_ATTRIBUTES :\n",
    "                    candidate_dataf = pd.get_dummies(candidate_dataf, columns=[candidate_feature])\n",
    "                \n",
    "                elif 'int' in candidate_feature_dtype or 'float' in candidate_feature_dtype or candidate_feature in  INTEGER_ATTRIBUTES :\n",
    "                    if candidate_feature_dtype == 'object':\n",
    "                        candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].replace(to_replace='NA', value=0)\n",
    "                        candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].astype(\"int\")\n",
    "                    # normalize in case of numeric dtype\n",
    "                    scaler = StandardScaler()\n",
    "                    candidate_dataf[[candidate_feature]] = scaler.fit_transform(candidate_dataf[[candidate_feature]])\n",
    "                \n",
    "                else: raise RuntimeError('Invalid dtype for candidate feature.')\n",
    "                \n",
    "                candidate_score_sample = self.datasetScoreSampler.compute_score_sample(candidate_dataf)\n",
    "                \n",
    "                # perform t-test for the means\n",
    "                #_, pvalue = stats.ttest_ind(candidate_score_sample, working_staged_score_sample, equal_var=False, alternative='greater')\n",
    "                #if pvalue < min_pvalue:\n",
    "                pvalue = mean(candidate_score_sample) - mean(working_staged_score_sample)\n",
    "                if pvalue > min_pvalue:\n",
    "                    min_pvalue = pvalue\n",
    "                    min_candidate_feature = candidate_feature\n",
    "                    min_candidate_score_sample = candidate_score_sample.copy()\n",
    "\n",
    "            if min_pvalue == 0.0:\n",
    "                break\n",
    "            candidate_feature = min_candidate_feature\n",
    "            outcome: bool = min_pvalue > 0  #min_pvalue < 0.05\n",
    "\n",
    "            if outcome:  # evidence to an increase in performance\n",
    "                include_features.append(candidate_feature)\n",
    "\n",
    "                candidate_dataf = working_staged_dataf.copy(deep=True)\n",
    "                candidate_dataf[min_candidate_feature] = self.global_df[min_candidate_feature]\n",
    "                if candidate_feature not in  INTEGER_ATTRIBUTES :\n",
    "                    candidate_dataf = pd.get_dummies(candidate_dataf, columns=[candidate_feature])\n",
    "                \n",
    "                elif 'int' in candidate_feature_dtype or 'float' in candidate_feature_dtype or candidate_feature in  INTEGER_ATTRIBUTES :\n",
    "                    if candidate_feature_dtype == 'object':\n",
    "                        candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].replace(to_replace='NA', value=0)\n",
    "                        candidate_dataf[candidate_feature] = candidate_dataf[candidate_feature].astype(\"int\")\n",
    "                    # normalize in case of numeric dtype\n",
    "                    scaler = StandardScaler()\n",
    "                    candidate_dataf[[candidate_feature]] = scaler.fit_transform(candidate_dataf[[candidate_feature]])\n",
    "                \n",
    "                else: raise RuntimeError('Invalid dtype for candidate feature.')\n",
    "                \n",
    "                working_staged_dataf = candidate_dataf\n",
    "                working_staged_score_sample = min_candidate_score_sample\n",
    "\n",
    "            self.print(\"%s selection - Validation outcome:\" % candidate_feature)\n",
    "            self.print(\"\\t- %s candidate\" % ('include' if outcome else 'discard'))\n",
    "            self.print(\"\\t- pvalue: %f\" % min_pvalue, \"\\n\")\n",
    "            self.print(\"\\t- current score: %f\" % mean(working_staged_score_sample))\n",
    "\n",
    "            candidate_features.remove(candidate_feature)\n",
    "        \n",
    "        final_score_mean = self.datasetScoreSampler.compute_score_mean(working_staged_dataf)\n",
    "        self.print(f\"Model name {self.model_name}\\nFinal dataframe score (avg): %f\" % final_score_mean, \"\\n\")\n",
    "        # self.print(\"Final dataset cols:\", str(working_staged_dataf.columns.tolist()))\n",
    "        \n",
    "        return include_features\n",
    "\n",
    "\n",
    "    def validate_backward(self, candidate_features: list[str], p_value_parm = 0.12) -> (list, list):\n",
    "\n",
    "        keep_features = []\n",
    "        drop_features = []\n",
    "        candidate_features = self.compute_corr_with_target(candidate_features, order='asc') \n",
    "\n",
    "        working_staged_dataf = self.staged_dataf.copy(deep=True)\n",
    "        working_staged_score_sample = self.staged_score_sample\n",
    "        working_pvalue = 1.0\n",
    "\n",
    "        for candidate_feature, corr in candidate_features:\n",
    "            if candidate_feature == TARGET_FEATURE or candidate_feature == TARGET_FEATURE_CONTI: continue\n",
    "\n",
    "            # candidate data frame (construction + score sample)\n",
    "            candidate_dataf = working_staged_dataf\n",
    "            cols_to_drop = []\n",
    "            for col in candidate_dataf.columns:\n",
    "                if col.startswith(candidate_feature): cols_to_drop.append(col)\n",
    "            candidate_dataf = candidate_dataf.drop(columns=cols_to_drop)\n",
    "\n",
    "            candidate_score_sample = self.datasetScoreSampler.compute_score_sample(candidate_dataf)\n",
    "            \n",
    "            # perform t-test for the means\n",
    "            print(\"CAND:\", candidate_score_sample)\n",
    "            print(\"WORK:\", working_staged_score_sample)\n",
    "            _, pvalue = stats.ttest_ind(candidate_score_sample, working_staged_score_sample, equal_var=False, alternative='less')\n",
    "            outcome: bool = pvalue < p_value_parm #and pvalue < working_pvalue\n",
    "            print(\"PVALUE:\", pvalue, \"\\n\")\n",
    "\n",
    "            # _, pvalue = stats.ttest_ind(candidate_score_sample, self.staged_score_sample, equal_var=False, alternative='greater')\n",
    "            # outcome: bool = pvalue > 0.05\n",
    "            \n",
    "            if not outcome:  # no evidence to a decrease in performance\n",
    "                drop_features.append(candidate_feature)\n",
    "                working_staged_dataf = candidate_dataf\n",
    "                working_staged_score_sample = candidate_score_sample\n",
    "                working_pvalue = pvalue\n",
    "            else:\n",
    "                keep_features.append(candidate_feature)\n",
    "            \n",
    "            self.print(\"%s selection - Validation outcome:\" % candidate_feature)\n",
    "            self.print(\"\\t- %s candidate\" % ('keep' if outcome else 'drop'))\n",
    "            self.print(\"\\t- pvalue: %f\" % pvalue, \"\\n\")\n",
    "\n",
    "\n",
    "        final_score_mean = self.datasetScoreSampler.compute_score_mean(working_staged_dataf)\n",
    "        self.print(\"Model name {self.model_name}\\nFinal dataframe score (avg): %f\" % final_score_mean, \"\\n\")\n",
    "        \n",
    "        return (drop_features, keep_features)\n",
    "\n",
    "\n",
    "    def print(self, *msg: str):\n",
    "        if not self.silent: print(' '.join(msg))\n",
    "    \n",
    "\n",
    "    def compute_corr_with_target(self, features: list[str], order: str='asc') -> list[float]:\n",
    "        if TARGET_FEATURE not in features:\n",
    "            features.append(TARGET_FEATURE)\n",
    "        corr_graph = understand_correlation_all(dataf=self.global_df, selected=features, threshold=0.0)\n",
    "        corr_with_target = corr_graph[features.index(TARGET_FEATURE)]\n",
    "        corr_with_target = corr_with_target[:-1]\n",
    "        features.remove(TARGET_FEATURE)\n",
    "\n",
    "        features_corrmap = {}\n",
    "        for f in features:\n",
    "            features_corrmap[f] = corr_with_target[features.index(f)]\n",
    "        reverse_order = order != 'asc'\n",
    "        return sorted(features_corrmap.items(), key=lambda x:x[1], reverse=reverse_order)\n",
    "\n",
    "\n",
    "\n",
    "def feature_selection_wrapper(global_df :  pd.DataFrame, df : pd.DataFrame, evaluation_model,  \n",
    "                            selected_att : list, discarded_features : list, parameter_eval=5 , evaluation_approach = 'kFold',\n",
    "                            metrics = ['balanced_accuracy'], niterations=5, silent: bool=True)->list:\n",
    "\n",
    "    # validate selected/discarded features\n",
    "    #\n",
    "    # \n",
    "    if evaluation_approach == 'kFold':    \n",
    "        scoreSampler = KFoldDatasetScoreSampler(k=parameter_eval, metrics=metrics, model=evaluation_model, niterations=niterations)\n",
    "    \n",
    "    fsv = FeatureSelectionValidator(global_df , df, scoreSampler, silent=silent)\n",
    "\n",
    "    (to_remove, to_keep) = fsv.validate_backward(selected_att)\n",
    "    print(\"Features to keep: \" + str(to_keep))\n",
    "    print(\"Features to remove: \" + str(to_remove))\n",
    "\n",
    "    df_transformed_noscale_wvb = df[to_keep + [TARGET_FEATURE]]\n",
    "    fsv.set_dataf(df_transformed_noscale_wvb)\n",
    "\n",
    "    to_add = fsv.validate_forward(discarded_features)\n",
    "    print(\"To add: \"+str(to_add))\n",
    "\n",
    "    return (to_keep, to_add)\n",
    "\n",
    "\n",
    "evaluation_model = ('DT', DecisionTreeClassifier(criterion='entropy', random_state=SEED))\n",
    "df_transformed_noscale = data_transformation(df_cleaned, aggregate_values=GOOD_FEATURES_AGGREGATED_VALUES, scale=False)\n",
    "staged_features = list(GOOD_FEATURES)\n",
    "unstaged_features = []\n",
    "for f in df.columns.tolist():\n",
    "    if f not in staged_features: unstaged_features.append(f)\n",
    "\n",
    "s = 0\n",
    "n = len(unstaged_features) - 1\n",
    "\n",
    "# use silent=True to hide the output\n",
    "#to_keep, to_add = feature_selection_wrapper(df, df_transformed_noscale, evaluation_model=evaluation_model,\n",
    "#                          selected_att=staged_features, discarded_features=unstaged_features[s:n],\n",
    "#                          parameter_eval=5, evaluation_approach='kFold', metrics=['accuracy'], niterations=10, silent=False)\n",
    "\n",
    "\n",
    "## CONSTANT KEEP - output after feature_selection_wrapper\n",
    "GOOD_FEATURES_TO_KEEP = ['HouseStyle', 'MSSubClass', 'FullBath', 'BsmtFinType1', 'YearBuilt', 'KitchenQual', 'Neighborhood', 'TotalSF']\n",
    "FEATURES_TO_ADD = ['GarageYrBlt']\n",
    "FINAL_FEATURES = GOOD_FEATURES_TO_KEEP + FEATURES_TO_ADD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation of final dataset with GOOD_FEATURES_TO_KEEP and FEATURES_TO_ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FEATURES_TYPE_MAP = { \"GarageYrBlt\" : [\"int64\"] }\n",
    "NEW_FEATURES_AGGREGATED_VALUES = {}\n",
    "\n",
    "\n",
    "def inconsistencies_new_features(df):\n",
    "    print(\"Incons. for GarageYrBlt:\")\n",
    "    print(f'If no garage, then no garageFinish {len(df[(df[\"GarageYrBlt\"] == \"NA\") & (df[\"GarageFinish\"] != \"NA\")])}')\n",
    "    print(f'If no garage, then no GarageType {len(df[(df[\"GarageYrBlt\"] == \"NA\") & (df[\"GarageType\"] != \"NA\")])}')\n",
    "    print(f'If no garage, then no garageArea {len(df[(df[\"GarageYrBlt\"] == \"NA\") & (df[\"GarageArea\"] != 0)])}')\n",
    "\n",
    "\n",
    "def data_cleaning_reduction_new_features(df: pd.DataFrame, selected_att : dict):\n",
    "    df = df.copy(deep=True)\n",
    "    #feature selection (era in data reduction, per facilita lo teniamo qua)\n",
    "    df = df.loc[:, selected_att.keys()]\n",
    "\n",
    "    #missing values there are no null values only NA\n",
    "    #df[\"attributo\"].fillna(\"valore\", inplace = True)\n",
    "\n",
    "    na_values = {\"GarageYrBlt\": [\"NA\", 0]}\n",
    "    for attribute in na_values.keys():\n",
    "        df[attribute] = df[attribute].replace(na_values[attribute][0], na_values[attribute][1]) \n",
    "    \n",
    "    #casting\n",
    "    for i in selected_att.keys():\n",
    "        if selected_att[i][0] == \"ordinal category\":\n",
    "            df[i] = df[i].astype(\"category\")\n",
    "            df[i] = df[i].cat.set_categories(selected_att[i][1], ordered=True)\n",
    "        elif selected_att[i][0] == \"ordinal integer\":\n",
    "            df[i]=df[i].astype(CategoricalDtype(ordered=True))\n",
    "        else:\n",
    "            df[i] = df[i].astype(selected_att[i][0])\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def data_transformation_new_features(df : pd.DataFrame, aggregate_values, scale = True):\n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    #Normalization\n",
    "    if scale:\n",
    "        type = [\"float64\", \"int\" ,\"int64\", \"float\"]\n",
    "        cols = df.select_dtypes(include = type).columns\n",
    "        scaler = StandardScaler()\n",
    "        df[cols] = scaler.fit_transform(df[cols])\n",
    "        \n",
    "    #Aggregation\n",
    "    for attribute in aggregate_values.keys():\n",
    "        for val in aggregate_values[attribute].keys():\n",
    "            df[attribute] = df[attribute].replace(val, aggregate_values[attribute][val])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def data_quality_cleaning_reduction_transformation_new_features(df, scale=True):\n",
    "    inconsistencies_new_features(df)\n",
    "    df_cleaned_new = data_cleaning_reduction_new_features(df, selected_att=NEW_FEATURES_TYPE_MAP)\n",
    "    df_transformed_new = data_transformation_new_features(df_cleaned_new, NEW_FEATURES_AGGREGATED_VALUES, scale=scale)\n",
    "    return df_transformed_new\n",
    "\n",
    "df_new_features_noscale = data_quality_cleaning_reduction_transformation_new_features(df, scale=False)\n",
    "df_final_noscale = pd.concat([df_transformed_noscale[GOOD_FEATURES_TO_KEEP + [TARGET_FEATURE]], df_new_features_noscale], axis=1)\n",
    "\n",
    "df_new_features = data_quality_cleaning_reduction_transformation_new_features(df, scale=True)\n",
    "df_final = pd.concat([df_transformed[GOOD_FEATURES_TO_KEEP + [TARGET_FEATURE]], df_new_features_noscale], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class OutlierAnalyzer:\n",
    "    # margin={'LOW': 0.015, 'MEDIUM': 0.01, 'HIGH': 0.05}\n",
    "    def __init__(self, dataf: pd.DataFrame, margin={'LOW': 0.01, 'MEDIUM': 0.01, 'HIGH': 0.01}, reg_margin=1.0e-18, silent: bool=False) -> None:\n",
    "        self.dataf = dataf\n",
    "        self.margin = margin\n",
    "        self.reg_margin = reg_margin\n",
    "        self.IQR_margin = 0.01\n",
    "        self.ncat_threshold = 25\n",
    "        self.silent = silent\n",
    "\n",
    "    def analyze(self, feature: str, skewed=False) -> list:\n",
    "        num_values, _ = self.__get_num_values(feature)\n",
    "        cat_values, _ = self.__get_cat_values(feature)\n",
    "\n",
    "        num_values_unique = set(num_values)\n",
    "        if len(num_values_unique) <= self.ncat_threshold:\n",
    "            cat_values += num_values\n",
    "            num_values = []\n",
    "        \n",
    "        if len(num_values) > 0:\n",
    "            lower_q = np.quantile(num_values, self.margin)\n",
    "            upper_q = np.quantile(num_values, 1.0 - self.margin)\n",
    "\n",
    "            Q1 = np.quantile(num_values, 0.25)  # TODO: in case of skewed distr.\n",
    "            Q3 = np.quantile(num_values, 0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_q = Q1 - 1.5*IQR\n",
    "            upper_q = Q3 + 1.5*IQR\n",
    "\n",
    "            outliers = []\n",
    "            for val in num_values:\n",
    "                if val < lower_q or val > upper_q: outliers.append(val)\n",
    "            print(\"Numerical outliers of %s:\" % feature, outliers)\n",
    "\n",
    "        if len(cat_values) > 0:\n",
    "            print(\"CAT VALS: \", cat_values)\n",
    "            freqs = collections.Counter(cat_values)\n",
    "            outliers = []\n",
    "            for val, freq in freqs.items():\n",
    "                freq = freq / len(cat_values)\n",
    "                if freq < self.margin or freq > 1.0 - self.margin: outliers.append(val)\n",
    "            print(\"Categorical outliers of %s:\" % feature, outliers)\n",
    "        return outliers\n",
    "    \n",
    "    def analyze_corr(self, feature: str, remove: bool=False, show_scplot: bool=False) -> list:\n",
    "        num_values, target_values = self.__get_num_values(feature)\n",
    "        num_values_unique = set(num_values)\n",
    "\n",
    "        outliers = set()\n",
    "        outliers_x = set()\n",
    "        outliers_y = set()\n",
    "        \n",
    "        num_outliers = []\n",
    "        target_values_cat = target_values.copy()\n",
    "        if len(num_values) > 0 and len(num_values_unique) > self.ncat_threshold:\n",
    "            \n",
    "            num_values_map = {'LOW': [], 'MEDIUM': [], 'HIGH': []}\n",
    "            for i in range(len(target_values)):\n",
    "                if   target_values[i] <= 150000: num_values_map['LOW'].append(num_values[i]); target_values_cat[i] = 'LOW'\n",
    "                elif target_values[i] <  300000: num_values_map['MEDIUM'].append(num_values[i]); target_values_cat[i] = 'MEDIUM'\n",
    "                else: num_values_map['HIGH'].append(num_values[i]); target_values_cat[i] = 'HIGH'\n",
    "            \n",
    "            for t_val in num_values_map.keys():\n",
    "                local_num_values = num_values_map[t_val]\n",
    "                if len(local_num_values) == 0: continue\n",
    "                \n",
    "                Q1 = np.quantile(local_num_values, self.IQR_margin)  # TODO: in case of skewed distr.\n",
    "                Q3 = np.quantile(local_num_values, 1.0 - self.IQR_margin)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_q = Q1 - 1.5*IQR\n",
    "                upper_q = Q3 + 1.5*IQR\n",
    "\n",
    "                local_num_outliers = []\n",
    "                for val in local_num_values:\n",
    "                    if val < lower_q or val > upper_q:\n",
    "                        num_outliers.append((val, t_val))\n",
    "                        local_num_outliers.append((val, t_val))\n",
    "                if not self.silent: print(\"Numerical outliers of %s-%s:\" % (feature, t_val), local_num_outliers)\n",
    "        \n",
    "        outliers = []\n",
    "        cat_values, cat_target_values = self.__get_cat_values(feature)\n",
    "\n",
    "        if len(num_values) > 0 and len(num_values) <= self.ncat_threshold:\n",
    "            cat_values += num_values\n",
    "            cat_target_values += target_values\n",
    "        \n",
    "        if len(cat_values) > 0:\n",
    "            crosstab = pd.crosstab(cat_values, cat_target_values)\n",
    "            \n",
    "            for rowIndex, row in crosstab.iterrows():\n",
    "                for columnIndex, value in row.items():\n",
    "                    #print(\"ROW\", crosstab.loc[rowIndex])\n",
    "                    total = crosstab.loc[rowIndex].sum()\n",
    "                    freq = value / total\n",
    "                    margin = self.margin[columnIndex]\n",
    "                    if freq > 0.0 and (freq < margin or freq > 1.0 - margin):\n",
    "                        outliers.append((rowIndex, columnIndex))\n",
    "        if not self.silent: print(\"Categorical-Corr outliers of %s:\" % feature, outliers)\n",
    "\n",
    "        outliers = list(num_outliers) + outliers\n",
    "\n",
    "        if show_scplot:\n",
    "            points_x = []\n",
    "            points_y = []\n",
    "\n",
    "            outliers_x = []\n",
    "            outliers_y = []\n",
    "            for i in range(len(num_values)):\n",
    "                if (num_values[i], target_values_cat[i]) in outliers:\n",
    "                    outliers_x.append(num_values[i])\n",
    "                    outliers_y.append(target_values[i])\n",
    "                else:\n",
    "                    points_x.append(num_values[i])\n",
    "                    points_y.append(target_values[i])\n",
    "            \n",
    "            plt.scatter(points_x, points_y, color='black')\n",
    "            plt.scatter(outliers_x, outliers_y, color='red')\n",
    "            plt.show()\n",
    "\n",
    "        if remove: self.__remove_corr_outliers(feature, outliers)\n",
    "        return outliers\n",
    "\n",
    "    def analyze_borders(self, remove: bool=False, margin: float=0.01) -> list:\n",
    "        \n",
    "        target_conti_values = self.dataf[TARGET_FEATURE_CONTI].tolist()\n",
    "        target_conti_low_values = []\n",
    "        target_conti_med_values = []\n",
    "        target_conti_hig_values = []\n",
    "\n",
    "        for v in target_conti_values:\n",
    "            if v <= 150000: target_conti_low_values.append(v)\n",
    "            elif v < 300000: target_conti_med_values.append(v)\n",
    "            else: target_conti_hig_values.append(v)\n",
    "        \n",
    "        q_low = np.quantile(target_conti_low_values, 1.0 - margin)\n",
    "        q_med_low = np.quantile(target_conti_med_values, margin)\n",
    "        q_med_hig = np.quantile(target_conti_med_values, 1.0 - margin)\n",
    "        q_hig = np.quantile(target_conti_hig_values, margin)\n",
    "\n",
    "        from random import random\n",
    "        outliers_indices = { 'LtoM': [], 'MtoL': [], 'MtoH': [], 'HtoM': [], 'NO': [] }\n",
    "        for i in range(len(target_conti_values)):\n",
    "            tv = target_conti_values[i]\n",
    "            #if random() < 0.0:\n",
    "            #    q_med_low: outliers_indices['MtoL'].append(i)\n",
    "            #    continue\n",
    "            if tv in target_conti_low_values and tv > q_low: outliers_indices['LtoM'].append(i)\n",
    "            elif tv in target_conti_med_values and tv < q_med_low: outliers_indices['MtoL'].append(i)\n",
    "            elif tv in target_conti_med_values and tv > q_med_hig: outliers_indices['MtoH'].append(i)\n",
    "            elif tv in target_conti_hig_values and tv < q_hig: outliers_indices['HtoM'].append(i)\n",
    "            else: outliers_indices['NO'].append(i)\n",
    "        \n",
    "        print(\"Border outliers:\", outliers_indices)\n",
    "\n",
    "        if remove:\n",
    "            #self.dataf = self.dataf.drop(outliers_indices, axis=0)\n",
    "            self.dataf['SPout'] = 'NO'\n",
    "            for catval, indices in outliers_indices.items():\n",
    "                if catval != 'NO':\n",
    "                    for i in indices: self.dataf.at[i, 'SPout'] = 'YES' #catval\n",
    "\n",
    "\n",
    "    def __get_num_values(self, feature: str) -> list:\n",
    "        feature_values = self.dataf[feature].tolist()\n",
    "        target_values = self.dataf[TARGET_FEATURE_CONTI].tolist()\n",
    "        f_values = []\n",
    "        t_values = []\n",
    "        for i in range(len(self.dataf.index)):\n",
    "            fval = feature_values[i]\n",
    "            try:\n",
    "                f_values.append(int(fval))\n",
    "                t_values.append(target_values[i])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return f_values, t_values\n",
    "    \n",
    "    def __get_cat_values(self, feature: str) -> list:\n",
    "        feature_values = self.dataf[feature].tolist()\n",
    "        target_values = self.dataf[TARGET_FEATURE].tolist()\n",
    "        f_values = []\n",
    "        t_values = []\n",
    "        for i in range(len(self.dataf.index)):\n",
    "            fval = feature_values[i]\n",
    "            if not str(fval).isnumeric():\n",
    "                f_values.append(str(fval))\n",
    "                t_values.append(target_values[i])\n",
    "        return f_values, t_values\n",
    "\n",
    "    def __remove_corr_outliers(self, feature: str, outliers: list[tuple]):\n",
    "        \n",
    "        outliers_indices_map = {}\n",
    "        for f_value, t_value in outliers:\n",
    "\n",
    "            target_feature = TARGET_FEATURE_CONTI if str(t_value).isnumeric() else TARGET_FEATURE\n",
    "            outlier_indices = self.dataf[ (self.dataf[feature] == f_value) & (self.dataf[target_feature] == t_value) ].index.tolist()\n",
    "            \n",
    "            if t_value == 'LOW' or t_value == 'MEDIUM':\n",
    "                #self.dataf.loc[ (self.dataf[feature] == f_value) & (self.dataf[target_feature] == t_value), target_feature ] = 'HIGH'\n",
    "                self.dataf = self.dataf.drop( outlier_indices )\n",
    "\n",
    "def outliers_analysis(dataf: pd.DataFrame, remove: bool=False) -> pd.DataFrame:\n",
    "    #\n",
    "    # remove outliers from dataf\n",
    "    #\n",
    "    df_with_outliers = dataf\n",
    "    df_with_outliers[TARGET_FEATURE_CONTI] = df[TARGET_FEATURE_CONTI]  # TODO: remove TARGET_FEATURE_CONTI (move to fit and validation)!\n",
    "\n",
    "    outlierAnalyzer = OutlierAnalyzer(df_with_outliers, silent=True)\n",
    "    noutliers = 0\n",
    "    for feature in FINAL_FEATURES:\n",
    "        if feature == TARGET_FEATURE or feature == TARGET_FEATURE_CONTI:\n",
    "            continue\n",
    "        outliers = outlierAnalyzer.analyze_corr(feature, remove=remove)\n",
    "        noutliers += len(outliers)\n",
    "        print(\"Outliers size of %s:\" % feature, len(outliers))\n",
    "\n",
    "    print(\"Total outliers:\", noutliers)\n",
    "\n",
    "    df_with_outliers = outlierAnalyzer.dataf\n",
    "    df_with_outliers = df_with_outliers.drop(TARGET_FEATURE_CONTI, axis=1)\n",
    "    df_with_outliers = df_with_outliers[FINAL_FEATURES + [TARGET_FEATURE]]\n",
    "\n",
    "    return df_with_outliers.index\n",
    "\n",
    "\n",
    "outliers_analysis(df_final_noscale, remove=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df_final: pd.DataFrame):\n",
    "    X_train_idx, X_test_idx, y_train_idx, y_test_idx = train_test_split(\n",
    "        df_final.index, df_final[TARGET_FEATURE].index, test_size=0.3, random_state=SEED)\n",
    "    return X_train_idx, X_test_idx\n",
    "\n",
    "# split training and test set\n",
    "train_idx, test_idx = split_train_test(df_final)\n",
    "\n",
    "# remove outliers (training set only)\n",
    "df_final_no_outliers_idx = outliers_analysis(df_final_noscale, remove=True)  # it returns a copy\n",
    "actual_train_idx = []\n",
    "for i in train_idx:\n",
    "    if i in df_final_no_outliers_idx:\n",
    "        actual_train_idx.append(i)\n",
    "\n",
    "# compute dummies for categorical features\n",
    "cat_attributes = list(df_final.select_dtypes(include = ['category', 'object']).columns)\n",
    "cat_attributes.remove(TARGET_FEATURE)\n",
    "df_final_dummies = pd.get_dummies(df_final, columns = cat_attributes)\n",
    "\n",
    "# create train set\n",
    "X_train = df_final_dummies.drop(TARGET_FEATURE, axis=1)\n",
    "y_train = df_final_dummies[TARGET_FEATURE]\n",
    "\n",
    "X_train = X_train.loc[actual_train_idx]\n",
    "y_train = y_train.loc[actual_train_idx]\n",
    "\n",
    "# create test set\n",
    "X_test = df_final_dummies.drop(TARGET_FEATURE, axis=1)\n",
    "y_test = df_final_dummies[TARGET_FEATURE]\n",
    "\n",
    "X_test = X_test.loc[test_idx]\n",
    "y_test = y_test.loc[test_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodels = []\n",
    "mymodels.append(('DT_EN',  DecisionTreeClassifier(criterion='entropy', random_state=SEED)))\n",
    "mymodels.append(('DT_GI', DecisionTreeClassifier(criterion='gini', random_state=SEED)))\n",
    "mymodels.append(('FRST', RandomForestClassifier(criterion='entropy', random_state=SEED)))\n",
    "mymodels.append(('FRST_BL', RandomForestClassifier(criterion='entropy', random_state=SEED, class_weight={\"HIGH\":10, \"LOW\":1, \"MEDIUM\":30})))\n",
    "mymodels.append(('ADAB', AdaBoostClassifier( random_state=SEED)))\n",
    "mymodels.append(('GRDB', GradientBoostingClassifier(criterion='squared_error', random_state=SEED)))\n",
    "mymodels.append(('GNB', GaussianNB())) \n",
    "#mymodels.append(('NN', MLPClassifier(hidden_layer_sizes=(50, 10, ), max_iter=500, verbose=False,  random_state=SEED)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit and Validation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = X_train.copy()\n",
    "data_train[TARGET_FEATURE] = y_train\n",
    "for name, model in mymodels:\n",
    "    scorer = KFoldDatasetScoreSampler(k=5, metrics=['accuracy'], niterations=20, model=(name, model))\n",
    "    print(\"Model name: \" + name + \", score: \" + str(scorer.compute_score_mean(dataf=data_train)))\n",
    "\n",
    "\"\"\"\n",
    "Model name: DT_EN, score: 0.7823052129333536\n",
    "Model name: DT_GI, score: 0.7868882290239074\n",
    "Model name: FRST, score: 0.8432904421095376\n",
    "Model name: FRST_BL, score: 0.8380506065681945\n",
    "Model name: ADAB, score: 0.7351822242525761\n",
    "Model name: GRDB, score: 0.833307953910969\n",
    "Model name: GNB, score: 0.6303362773463276\n",
    "Model name: NN, score: 0.799215522054718\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_DT =  DecisionTreeClassifier(criterion='entropy', random_state=SEED)\n",
    "final_model_RFS = RandomForestClassifier(criterion='entropy', random_state=SEED)\n",
    "final_model_RFS_BL = RandomForestClassifier(criterion='entropy', random_state=1020, class_weight={\"HIGH\":10, \"LOW\":1, \"MEDIUM\":30})\n",
    "\n",
    "models_choosed = {\n",
    "    'DT': final_model_DT,\n",
    "    'RFS': final_model_RFS,\n",
    "    'RFS_BL': final_model_RFS_BL\n",
    "}\n",
    "\n",
    "models_choosed_pred = {}\n",
    "\n",
    "## train and test\n",
    "for model_name, model in models_choosed.items():\n",
    "    \n",
    "    model.fit(X=X_train,y=y_train)\n",
    "    y_pred = model.predict(X=X_test)\n",
    "    models_choosed_pred[model_name] = y_pred\n",
    "    \n",
    "    print(f\"Classification report {model_name}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf, categories='auto', cbar=True, cmap='Blues', title=None):\n",
    "    group_counts = [f'{value}\\n' for value in cf.flatten()]\n",
    "\n",
    "    box_labels = [f'{v1}'.strip() for v1 in group_counts]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "    sb.heatmap(cf, annot=box_labels, fmt='', cmap=cmap, cbar=cbar, xticklabels=categories, yticklabels=categories)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for model_name, model in models_choosed.items():\n",
    "    print(f\"Classification report {model_name}\")\n",
    "    make_confusion_matrix(confusion_matrix(y_test, models_choosed_pred[model_name]), categories=TARGET_FEATURE_VALUES, cmap='binary')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = X_test.copy()\n",
    "data_test[TARGET_FEATURE] = y_test\n",
    "one_hot_encoding = np.array(pd.get_dummies(data_test, columns = ['SalePrice'])[['SalePrice_HIGH','SalePrice_LOW', 'SalePrice_MEDIUM']])\n",
    "\n",
    "fpr = []\n",
    "tpr = []\n",
    "roc_auc = dict()\n",
    "\n",
    "CLASSES = ['HIGH', 'LOW', 'MEDIUM']\n",
    "\n",
    "for i in range(len(CLASSES)):\n",
    "    plt.figure()\n",
    "\n",
    "    for name, model in models_choosed.items():\n",
    "        probabilities = model.predict_proba(X=X_test)\n",
    "        fpr, tpr , _ = roc_curve(one_hot_encoding[:, i], probabilities[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=name + f' (area = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Class = ' + str(CLASSES[i]))\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_att = {\"MSSubClass\": [\"ordinal integer\"],\n",
    "                \"Neighborhood\": [\"category\"],\n",
    "                \"HouseStyle\": [\"category\"],\n",
    "                \"OverallQual\": [\"ordinal integer\"],\n",
    "                \"YearBuilt\": [\"int64\"],\n",
    "                \"BsmtFinType1\":[\"ordinal category\", [\"NA\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"]],\n",
    "                \"FullBath\":[\"int64\"],\n",
    "                \"KitchenQual\":[\"ordinal category\", [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"]],\n",
    "                \"TotalSF\": [\"int64\"],\n",
    "                \"GarageArea\": [\"int64\"],\n",
    "                \"GrLivArea\": [\"int64\"],\n",
    "                \"CentralAir\": [\"category\"],\n",
    "                \"MasVnrArea\": [\"int64\"],\n",
    "                \"OverallQual\": [\"int64\"],\n",
    "                \"MSZoning\": [\"category\"],\n",
    "                \"PavedDrive\": [\"category\"],\n",
    "                \"GarageCond\": [\"category\"],\n",
    "                \"GarageCars\": [\"category\"],\n",
    "                \"GarageFinish\": [\"category\"],\n",
    "                \"KitchenQual\": [\"category\"],\n",
    "                \"Utilities\": [\"category\"],\n",
    "                \"ScreenPorch\": [\"int64\"],\n",
    "                \"Electrical\": [\"category\"],\n",
    "                \"LandSlope\": [\"category\"],\n",
    "                \"SalePrice\":[\"category\"]}\n",
    "\n",
    "\n",
    "aggregate_values = {}\n",
    "\n",
    "#SELECTED_FEATURES = ['TotalSF', 'OverallQual', 'Neighborhood', 'ScreenPorch', 'HouseStyle', 'Electrical', 'LandSlope']\n",
    "\n",
    "\n",
    "SELECTED_FEATURES = ['TotalSF', 'GarageArea', 'GrLivArea', 'CentralAir', 'MasVnrArea', 'OverallQual', 'MSZoning', 'PavedDrive', 'GarageCond', 'GarageCars',\n",
    "                     'GarageFinish', 'KitchenQual', 'Utilities', 'MSSubClass', 'LotArea', 'Condition1', 'HouseStyle', 'OverallCond',\n",
    "                     'BsmtQual', 'SaleType',\n",
    "                     'Fireplaces', 'Functional']\n",
    "\"\"\"\n",
    "aggregate_values = {\n",
    "                    \"HouseStyle\": {\n",
    "                                    \"1Story\" : \"other\",\n",
    "                                    \"1.5Unf\" : \"other\",\n",
    "                                    \"2.5Fin\" : \"other\",\n",
    "                                    \"2.5Unf\" : \"other\",\n",
    "                                    \"Sfoyer\" : \"other\",\n",
    "                                    \"SLvl\" : \"other\"\n",
    "                                    },\n",
    "                    \"OverallQual\": {\n",
    "                                    1 : 4,\n",
    "                                    2 : 4,\n",
    "                                    3 : 4,\n",
    "                                    10 : 9,\n",
    "                                    },\n",
    "                    \"BsmtFinType1\": {\n",
    "                                    \"ALQ\" : \"other\",\n",
    "                                    \"BLQ\" : \"other\",\n",
    "                                    \"Rec\" : \"other\",\n",
    "                                    \"LwQ\" : \"other\",\n",
    "                                    \"NA\" : \"other\",\n",
    "                                },\n",
    "                    \"KitchenQual\": {\n",
    "                                    \"Fa\" : \"TA\",\n",
    "                                    \"Po\" : \"TA\",\n",
    "                                },\n",
    "                    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataf: pd.DataFrame, global_df:pd.DataFrame=None, for_training: bool=True, training_features: list=None) -> pd.DataFrame:\n",
    "    \n",
    "    #\n",
    "    # feature creation (fcr) only\n",
    "    dataf = data_transformation(dataf, aggregate_values, scale=False, aggr=False, fcr=True)\n",
    "\n",
    "    #\n",
    "    # feature selection\n",
    "    dataf = dataf[SELECTED_FEATURES + [TARGET_FEATURE]]\n",
    "\n",
    "    #\n",
    "    # data cleaning\n",
    "    #dataf = data_cleaning_reduction(dataf, selected_att=selected_att)\n",
    "\n",
    "    #\n",
    "    # remove outliers from training_df\n",
    "    no_outliers_indices = dataf.index\n",
    "    if for_training:\n",
    "        df_with_outliers = dataf\n",
    "        outlierAnalyzer = OutlierAnalyzer(df_with_outliers, silent=True)\n",
    "\n",
    "        dataf[TARGET_FEATURE_CONTI] = global_df[TARGET_FEATURE_CONTI]\n",
    "        #for feature in dataf.columns.tolist():\n",
    "        #    if feature == TARGET_FEATURE or feature == TARGET_FEATURE_CONTI: continue\n",
    "            #outliers = outlierAnalyzer.analyze_corr(feature, remove=True)\n",
    "        outliers = outlierAnalyzer.analyze_borders(remove=True, margin=0.2)\n",
    "        \n",
    "        no_outliers_indices = outlierAnalyzer.dataf.index\n",
    "        dataf = dataf.drop(TARGET_FEATURE_CONTI, axis=1)\n",
    "        print(\"No outlier rows: %d\" % len(no_outliers_indices))\n",
    "        print(\"Border rows: %d\" % (1460 - len(no_outliers_indices)))\n",
    "    \n",
    "    #\n",
    "    # data transformation (scaling and aggregation only)\n",
    "    dataf = data_transformation(dataf, aggregate_values, scale=True, aggr=True, fcr=False)\n",
    "\n",
    "    #\n",
    "    # compute dummies\n",
    "    cat_attributes = list(dataf.select_dtypes(include = ['category', 'object']).columns)\n",
    "    if TARGET_FEATURE in cat_attributes: cat_attributes.remove(TARGET_FEATURE)\n",
    "    dataf = pd.get_dummies(dataf, columns=cat_attributes)\n",
    "\n",
    "    #\n",
    "    # remove outlier rows\n",
    "    #\n",
    "    dataf = dataf.loc[no_outliers_indices]\n",
    "\n",
    "    #print(\"\\n\\n\\n\\nDATAFRAME_FINAL\\n\", dataf.head())\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def train_model(training_df: pd.DataFrame, global_df:pd.DataFrame, train_idx: np.ndarray, model): # training_df must be untouched (no dummies, no scale)\n",
    "    \n",
    "    training_df = prepare_dataset(training_df, global_df, for_training=False)\n",
    "\n",
    "    #\n",
    "    # train the model on the training set\n",
    "    actual_train_idx = []\n",
    "    for i in train_idx:\n",
    "        if i in training_df.index: actual_train_idx.append(i)\n",
    "    \n",
    "    X_train = training_df.drop(TARGET_FEATURE, axis=1)\n",
    "    y_train = training_df[TARGET_FEATURE]\n",
    "\n",
    "    X_train = X_train.loc[actual_train_idx]\n",
    "    y_train = y_train.loc[actual_train_idx]\n",
    "    \n",
    "    print(\"Training attrs:\", X_train.columns.tolist())\n",
    "    print(\"Training length:\", len(X_train))\n",
    "\n",
    "    model.fit(X=X_train.to_numpy(), y=y_train.to_numpy())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def classify(test_df: pd.DataFrame, global_df: pd.DataFrame, test_idx: np.ndarray, model):\n",
    "    \n",
    "    test_df = prepare_dataset(test_df, global_df, for_training=False)\n",
    "\n",
    "    actual_test_idx = []\n",
    "    for i in test_idx:\n",
    "        if i in test_df.index: actual_test_idx.append(i)\n",
    "\n",
    "    X_test = test_df.drop(TARGET_FEATURE, axis=1)\n",
    "    y_test = test_df[TARGET_FEATURE]\n",
    "\n",
    "    X_test = X_test.loc[actual_test_idx].to_numpy()\n",
    "    y_test = y_test.loc[actual_test_idx].to_numpy()\n",
    "\n",
    "    print(\"Testing length:\", len(X_test))\n",
    "\n",
    "    y_pred = model.predict(X=X_test)\n",
    "\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# simulation of a brand new training set and test set\n",
    "#\n",
    "#####################################################\n",
    "global_df = df  # dataset given in input\n",
    "df_copy = global_df.copy(deep=True)\n",
    "features = global_df.columns.tolist()\n",
    "features.remove(TARGET_FEATURE)\n",
    "features.remove(TARGET_FEATURE_CONTI)\n",
    "\n",
    "train_idx, test_idx = split_train_test(global_df[features + [TARGET_FEATURE]])\n",
    "#####################################################\n",
    "\n",
    "# simulation of an execution\n",
    "model = RandomForestClassifier(criterion='entropy', random_state=SEED)\n",
    "\n",
    "model = train_model(df_copy, global_df, train_idx, model)\n",
    "y_test, y_pred = classify(df_copy, global_df, test_idx, model)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "make_confusion_matrix(confusion_matrix(y_test, y_pred), categories= ['HIGH', 'LOW', 'MEDIUM'] , cmap='binary')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KDD99-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
